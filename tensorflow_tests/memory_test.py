"""
TensorFlow ë©”ëª¨ë¦¬ ì§‘ì•½ì  í…ŒìŠ¤íŠ¸
ì´ í…ŒìŠ¤íŠ¸ëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìµœëŒ€í™”í•˜ì—¬ ë©”ëª¨ë¦¬ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import tensorflow as tf
import numpy as np
import time
import argparse
import sys
import os
import gc

# Add utils directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.resource_monitor import ResourceMonitor, print_system_info


class TensorFlowMemoryConsumer:
    """TensorFlow ë©”ëª¨ë¦¬ ì†Œë¹„ì í´ë˜ìŠ¤"""
    def __init__(self):
        self.tensor_storage = []
        self.model_storage = []
        
        # CPUë§Œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
        tf.config.set_visible_devices([], 'GPU')
        print("TensorFlow ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ - CPU ëª¨ë“œë¡œ ì„¤ì •ë¨")
    
    def allocate_large_tensors(self, num_tensors=100, tensor_shape=(5000, 5000)):
        """ëŒ€ëŸ‰ì˜ í…ì„œë¥¼ ë©”ëª¨ë¦¬ì— í• ë‹¹"""
        print(f"ëŒ€ìš©ëŸ‰ í…ì„œ í• ë‹¹: {num_tensors}ê°œ, í¬ê¸°: {tensor_shape}")
        
        tensors = []
        for i in range(num_tensors):
            # ë‹¤ì–‘í•œ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ í…ì„œ ìƒì„±
            if i % 4 == 0:
                tensor = tf.random.normal(tensor_shape, dtype=tf.float32)
            elif i % 4 == 1:
                tensor = tf.random.normal(tensor_shape, dtype=tf.float64)
            elif i % 4 == 2:
                tensor = tf.random.uniform(tensor_shape, 0, 100, dtype=tf.int32)
            else:
                tensor = tf.ones(tensor_shape, dtype=tf.float32)
            
            tensors.append(tensor)
            
            if i % 10 == 0:
                print(f"  í• ë‹¹ë¨: {i+1}/{num_tensors}")
        
        self.tensor_storage.extend(tensors)
        return tensors
    
    def create_memory_intensive_models(self, num_models=5):
        """ë©”ëª¨ë¦¬ ì§‘ì•½ì  ëª¨ë¸ë“¤ ìƒì„±"""
        print(f"ë©”ëª¨ë¦¬ ì§‘ì•½ì  ëª¨ë¸ {num_models}ê°œ ìƒì„±")
        
        models = []
        for i in range(num_models):
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(4096, activation='relu', input_shape=(10000,)),
                tf.keras.layers.Dense(2048, activation='relu'),
                tf.keras.layers.Dense(1024, activation='relu'),
                tf.keras.layers.Dense(512, activation='relu'),
                tf.keras.layers.Dense(256, activation='relu'),
                tf.keras.layers.Dense(128, activation='relu'),
                tf.keras.layers.Dense(64, activation='relu'),
                tf.keras.layers.Dense(10, activation='softmax')
            ])
            
            # ëª¨ë¸ ì»´íŒŒì¼
            model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
            
            # ë”ë¯¸ ë°ì´í„°ë¡œ ëª¨ë¸ ì´ˆê¸°í™”
            dummy_input = tf.random.normal([1, 10000])
            _ = model(dummy_input)
            
            models.append(model)
            print(f"  ëª¨ë¸ {i+1} ìƒì„± ì™„ë£Œ")
        
        self.model_storage.extend(models)
        return models
    
    def generate_large_datasets(self, num_samples=100000, feature_size=5000):
        """ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ìƒì„±"""
        print(f"ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ìƒì„±: {num_samples} ìƒ˜í”Œ x {feature_size} íŠ¹ì„±")
        
        # ê¸°ë³¸ ë°ì´í„°
        X = tf.random.normal([num_samples, feature_size])
        y = tf.random.uniform([num_samples], 0, 1000, dtype=tf.int32)
        
        # ë°ì´í„° ë³€í™˜ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€
        X_normalized = tf.nn.l2_normalize(X, axis=1)
        X_squared = tf.square(X)
        X_log = tf.math.log(tf.abs(X) + 1e-8)
        X_combined = tf.concat([X, X_normalized, X_squared, X_log], axis=1)
        
        dataset = {
            'X': X,
            'y': y,
            'X_normalized': X_normalized,
            'X_squared': X_squared,
            'X_log': X_log,
            'X_combined': X_combined
        }
        
        return dataset
    
    def progressive_memory_allocation(self, max_iterations=100):
        """ì ì§„ì  ë©”ëª¨ë¦¬ í• ë‹¹"""
        print(f"ì ì§„ì  ë©”ëª¨ë¦¬ í• ë‹¹: {max_iterations} ë°˜ë³µ")
        
        current_size = 1000
        allocated_tensors = []
        
        for i in range(max_iterations):
            try:
                tensor_size = int(current_size)
                tensor = tf.random.normal([tensor_size, tensor_size])
                allocated_tensors.append(tensor)
                
                current_size *= 1.1
                
                if i % 10 == 0:
                    print(f"  ë°˜ë³µ {i}: í¬ê¸° {tensor_size}x{tensor_size}")
                    
            except Exception as e:
                print(f"  ë©”ëª¨ë¦¬ í•œê³„ ë„ë‹¬: ë°˜ë³µ {i}")
                break
        
        return allocated_tensors
    
    def memory_leak_simulation(self, iterations=50):
        """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì‹œë®¬ë ˆì´ì…˜"""
        print(f"ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì‹œë®¬ë ˆì´ì…˜: {iterations} ë°˜ë³µ")
        
        leaked_tensors = []
        
        for i in range(iterations):
            # ëŒ€ìš©ëŸ‰ í…ì„œ ìƒì„±
            tensor = tf.random.normal([3000, 3000])
            
            # ë³µì¡í•œ ì—°ì‚° ìˆ˜í–‰
            result = tf.matmul(tensor, tf.transpose(tensor))
            result = tf.sin(result) + tf.cos(result)
            result = tf.exp(tf.clip_by_value(result, -3, 3))
            
            # ê²°ê³¼ ì €ì¥ (ëˆ„ìˆ˜ ì‹œë®¬ë ˆì´ì…˜)
            leaked_tensors.append(result)
            
            if i % 10 == 0:
                print(f"  ëˆ„ìˆ˜ ì‹œë®¬ë ˆì´ì…˜ {i}/{iterations}")
        
        return leaked_tensors
    
    def clear_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        print("ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
        self.tensor_storage.clear()
        self.model_storage.clear()
        gc.collect()


def batch_processing_memory_test(batch_size=1000, num_batches=100):
    """ë°°ì¹˜ ì²˜ë¦¬ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸"""
    print(f"ë°°ì¹˜ ì²˜ë¦¬ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸: {num_batches} ë°°ì¹˜, í¬ê¸°: {batch_size}")
    
    # ë©”ëª¨ë¦¬ ì§‘ì•½ì  ëª¨ë¸
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(2048, activation='relu', input_shape=(3000,)),
        tf.keras.layers.Dense(1024, activation='relu'),
        tf.keras.layers.Dense(512, activation='relu'),
        tf.keras.layers.Dense(256, activation='relu'),
        tf.keras.layers.Dense(100, activation='softmax')
    ])
    
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
    
    for batch_idx in range(num_batches):
        # ë°°ì¹˜ ë°ì´í„° ìƒì„±
        X = tf.random.normal([batch_size, 3000])
        y = tf.random.uniform([batch_size], 0, 100, dtype=tf.int32)
        
        # í•™ìŠµ ìŠ¤í…
        with tf.GradientTape() as tape:
            predictions = model(X, training=True)
            loss = tf.keras.losses.sparse_categorical_crossentropy(y, predictions)
            loss = tf.reduce_mean(loss)
        
        gradients = tape.gradient(loss, model.trainable_variables)
        model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))
        
        if batch_idx % 20 == 0:
            print(f"  ë°°ì¹˜ {batch_idx}/{num_batches}")


def main():
    parser = argparse.ArgumentParser(description='TensorFlow ë©”ëª¨ë¦¬ ì§‘ì•½ì  í…ŒìŠ¤íŠ¸')
    parser.add_argument('--duration', type=int, default=120, help='í…ŒìŠ¤íŠ¸ ì§€ì† ì‹œê°„ (ì´ˆ)')
    parser.add_argument('--num-tensors', type=int, default=50, help='í• ë‹¹í•  í…ì„œ ìˆ˜')
    parser.add_argument('--tensor-size', type=int, default=3000, help='í…ì„œ í¬ê¸°')
    parser.add_argument('--num-models', type=int, default=3, help='ìƒì„±í•  ëª¨ë¸ ìˆ˜')
    parser.add_argument('--skip-progressive', action='store_true', help='ì ì§„ì  í• ë‹¹ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--skip-leak', action='store_true', help='ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°')
    
    args = parser.parse_args()
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    print_system_info()
    
    # TensorFlow ì •ë³´
    print(f"\nTensorFlow ë²„ì „: {tf.__version__}")
    
    # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘
    monitor = ResourceMonitor(interval=0.5)
    monitor.start_monitoring()
    
    print(f"\nğŸ§  TensorFlow ë©”ëª¨ë¦¬ ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ ì‹œì‘ (ì§€ì† ì‹œê°„: {args.duration}ì´ˆ)")
    print("=" * 60)
    
    memory_consumer = TensorFlowMemoryConsumer()
    start_time = time.time()
    
    try:
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        
        # 1. ëŒ€ëŸ‰ í…ì„œ í• ë‹¹
        print("\n1. ëŒ€ëŸ‰ í…ì„œ í• ë‹¹")
        tensors = memory_consumer.allocate_large_tensors(
            num_tensors=args.num_tensors,
            tensor_shape=(args.tensor_size, args.tensor_size)
        )
        
        # 2. ë©”ëª¨ë¦¬ ì§‘ì•½ì  ëª¨ë¸ ìƒì„±
        print("\n2. ë©”ëª¨ë¦¬ ì§‘ì•½ì  ëª¨ë¸ ìƒì„±")
        models = memory_consumer.create_memory_intensive_models(num_models=args.num_models)
        
        # 3. ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ìƒì„±
        print("\n3. ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ìƒì„±")
        dataset = memory_consumer.generate_large_datasets(num_samples=30000, feature_size=3000)
        
        # 4. ì ì§„ì  ë©”ëª¨ë¦¬ í• ë‹¹
        if not args.skip_progressive:
            print("\n4. ì ì§„ì  ë©”ëª¨ë¦¬ í• ë‹¹")
            progressive_tensors = memory_consumer.progressive_memory_allocation(max_iterations=50)
        
        # 5. ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì‹œë®¬ë ˆì´ì…˜
        if not args.skip_leak:
            print("\n5. ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì‹œë®¬ë ˆì´ì…˜")
            leaked_tensors = memory_consumer.memory_leak_simulation(iterations=30)
        
        # 6. ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        print("\n6. ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
        batch_processing_memory_test(batch_size=500, num_batches=50)
        
        # ë‚¨ì€ ì‹œê°„ ë™ì•ˆ ì¶”ê°€ í…ì„œ í• ë‹¹
        elapsed_time = time.time() - start_time
        remaining_time = args.duration - elapsed_time
        
        if remaining_time > 10:
            print(f"\n7. ì¶”ê°€ í…ì„œ í• ë‹¹ ({remaining_time:.1f}ì´ˆ ë™ì•ˆ)")
            extra_tensors = memory_consumer.allocate_large_tensors(
                num_tensors=max(10, int(remaining_time / 5)),
                tensor_shape=(2000, 2000)
            )
        
    except KeyboardInterrupt:
        print("\ní…ŒìŠ¤íŠ¸ê°€ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\ní…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
        monitor.stop_monitoring()
        monitor.print_summary()
        
        # ìµœì¢… ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
        final_usage = monitor.get_current_usage()
        print(f"\nìµœì¢… ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰:")
        print(f"Memory: {final_usage.get('memory_percent', 0):.1f}%")
        print(f"Memory MB: {final_usage.get('memory_mb', 0):.1f} MB")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        memory_consumer.clear_memory()
        
        print("\nğŸ¯ TensorFlow ë©”ëª¨ë¦¬ ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("=" * 60)


if __name__ == "__main__":
    main() 