"""
TensorFlow GPU ë©”ëª¨ë¦¬ ì§‘ì•½ì  í…ŒìŠ¤íŠ¸
ì´ í…ŒìŠ¤íŠ¸ëŠ” GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìµœëŒ€í™”í•˜ì—¬ GPU ë©”ëª¨ë¦¬ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import tensorflow as tf
import numpy as np
import time
import argparse
import sys
import os

# Add utils directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.resource_monitor import ResourceMonitor, print_system_info


def setup_tensorflow_gpu():
    """TensorFlow GPU ì„¤ì •"""
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        try:
            # GPU ë©”ëª¨ë¦¬ ì„±ì¥ í—ˆìš©
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"GPU ì„¤ì • ì™„ë£Œ: {len(gpus)}ê°œ GPU ì‚¬ìš© ê°€ëŠ¥")
            return True
        except RuntimeError as e:
            print(f"GPU ì„¤ì • ì˜¤ë¥˜: {e}")
            return False
    else:
        print("âš ï¸  GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        return False


def allocate_large_gpu_tensors(num_tensors=50, tensor_shape=(4000, 4000)):
    """ëŒ€ëŸ‰ì˜ GPU í…ì„œ í• ë‹¹"""
    if not tf.config.list_physical_devices('GPU'):
        print("GPUê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return []
    
    print(f"GPUì— ëŒ€ìš©ëŸ‰ í…ì„œ í• ë‹¹: {num_tensors}ê°œ, í¬ê¸°: {tensor_shape}")
    
    with tf.device('/GPU:0'):
        tensors = []
        for i in range(num_tensors):
            try:
                # ë‹¤ì–‘í•œ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ GPU í…ì„œ ìƒì„±
                if i % 3 == 0:
                    tensor = tf.random.normal(tensor_shape, dtype=tf.float32)
                elif i % 3 == 1:
                    tensor = tf.random.normal(tensor_shape, dtype=tf.float16)
                else:
                    tensor = tf.random.uniform(tensor_shape, 0, 100, dtype=tf.int32)
                
                tensors.append(tensor)
                
                if i % 10 == 0:
                    print(f"  í• ë‹¹ë¨: {i+1}/{num_tensors}")
                    
            except tf.errors.ResourceExhaustedError:
                print(f"  GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ {i}ë²ˆì§¸ í…ì„œì—ì„œ ì¤‘ë‹¨")
                break
            except Exception as e:
                print(f"  ì˜¤ë¥˜ ë°œìƒ: {e}")
                break
    
    return tensors


def create_massive_gpu_model():
    """ëŒ€ìš©ëŸ‰ GPU ëª¨ë¸ ìƒì„±"""
    if not tf.config.list_physical_devices('GPU'):
        print("GPUê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None
    
    print("ëŒ€ìš©ëŸ‰ GPU ëª¨ë¸ ìƒì„±")
    
    with tf.device('/GPU:0'):
        try:
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(8192, activation='relu', input_shape=(20000,)),
                tf.keras.layers.Dense(4096, activation='relu'),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.Dense(2048, activation='relu'),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.Dense(1024, activation='relu'),
                tf.keras.layers.Dense(512, activation='relu'),
                tf.keras.layers.Dense(256, activation='relu'),
                tf.keras.layers.Dense(128, activation='relu'),
                tf.keras.layers.Dense(10, activation='softmax')
            ])
            
            # ëª¨ë¸ ì»´íŒŒì¼
            model.compile(
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy']
            )
            
            # ë”ë¯¸ ì…ë ¥ìœ¼ë¡œ ëª¨ë¸ ì´ˆê¸°í™”
            dummy_input = tf.random.normal([1, 20000])
            _ = model(dummy_input)
            
            # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
            total_params = model.count_params()
            print(f"  ëª¨ë¸ ìƒì„± ì™„ë£Œ: {total_params:,} íŒŒë¼ë¯¸í„°")
            
            return model
            
        except tf.errors.ResourceExhaustedError:
            print("  GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨")
            return None
        except Exception as e:
            print(f"  ëª¨ë¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return None


def progressive_gpu_memory_allocation(max_iterations=100):
    """ì ì§„ì  GPU ë©”ëª¨ë¦¬ í• ë‹¹"""
    if not tf.config.list_physical_devices('GPU'):
        print("GPUê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return []
    
    print(f"ì ì§„ì  GPU ë©”ëª¨ë¦¬ í• ë‹¹: {max_iterations} ë°˜ë³µ")
    
    with tf.device('/GPU:0'):
        current_size = 1000
        allocated_tensors = []
        
        for i in range(max_iterations):
            try:
                tensor_size = int(current_size)
                tensor = tf.random.normal([tensor_size, tensor_size], dtype=tf.float32)
                allocated_tensors.append(tensor)
                
                current_size *= 1.05
                
                if i % 10 == 0:
                    print(f"  ë°˜ë³µ {i}: í¬ê¸° {tensor_size}x{tensor_size}")
                    
            except tf.errors.ResourceExhaustedError:
                print(f"  GPU ë©”ëª¨ë¦¬ í•œê³„ì— ë„ë‹¬: ë°˜ë³µ {i}")
                break
            except Exception as e:
                print(f"  ì˜¤ë¥˜ ë°œìƒ: {e}")
                break
        
        return allocated_tensors


def gpu_memory_stress_test(stress_level=3):
    """GPU ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸"""
    if not tf.config.list_physical_devices('GPU'):
        print("GPUê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    
    print(f"GPU ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ (ë ˆë²¨: {stress_level})")
    
    with tf.device('/GPU:0'):
        base_size = 2000
        tensor_size = base_size * stress_level
        
        stress_tensors = []
        
        for i in range(20):
            try:
                # ë‹¤ì–‘í•œ ì—°ì‚°ìœ¼ë¡œ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©
                a = tf.random.normal([tensor_size, tensor_size])
                b = tf.random.normal([tensor_size, tensor_size])
                
                # í–‰ë ¬ ê³±ì…ˆ
                c = tf.matmul(a, b)
                
                # ì¶”ê°€ ì—°ì‚°
                d = tf.sin(c) + tf.cos(c)
                e = tf.matmul(d, tf.transpose(a))
                
                stress_tensors.append(e)
                
                if i % 5 == 0:
                    print(f"  ìŠ¤íŠ¸ë ˆìŠ¤ {i}: í¬ê¸° {tensor_size}x{tensor_size}")
                    
            except tf.errors.ResourceExhaustedError:
                print(f"  ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì¤‘ ë©”ëª¨ë¦¬ ë¶€ì¡±: ë‹¨ê³„ {i}")
                break
            except Exception as e:
                print(f"  ì˜¤ë¥˜ ë°œìƒ: {e}")
                break
        
        return stress_tensors


def gpu_training_memory_test(batch_size=128, num_epochs=20):
    """GPU í•™ìŠµ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸"""
    if not tf.config.list_physical_devices('GPU'):
        print("GPUê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    
    print(f"GPU í•™ìŠµ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ (ë°°ì¹˜: {batch_size}, ì—í¬í¬: {num_epochs})")
    
    with tf.device('/GPU:0'):
        # ë©”ëª¨ë¦¬ ì§‘ì•½ì  ëª¨ë¸
        model = tf.keras.Sequential([
            tf.keras.layers.Dense(4096, activation='relu', input_shape=(5000,)),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Dense(2048, activation='relu'),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Dense(1024, activation='relu'),
            tf.keras.layers.Dense(512, activation='relu'),
            tf.keras.layers.Dense(100, activation='softmax')
        ])
        
        model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        # í•™ìŠµ ë£¨í”„
        for epoch in range(num_epochs):
            try:
                for batch in range(10):  # ì—í¬í¬ë‹¹ 10 ë°°ì¹˜
                    # ë°°ì¹˜ ë°ì´í„° ìƒì„±
                    X = tf.random.normal([batch_size, 5000])
                    y = tf.random.uniform([batch_size], 0, 100, dtype=tf.int32)
                    
                    # í•™ìŠµ ìŠ¤í…
                    with tf.GradientTape() as tape:
                        predictions = model(X, training=True)
                        loss = tf.keras.losses.sparse_categorical_crossentropy(y, predictions)
                        loss = tf.reduce_mean(loss)
                    
                    gradients = tape.gradient(loss, model.trainable_variables)
                    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))
                
                if epoch % 5 == 0:
                    print(f"  ì—í¬í¬ {epoch}: ì†ì‹¤ {loss.numpy():.4f}")
                    
            except tf.errors.ResourceExhaustedError:
                print(f"  GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì—í¬í¬ {epoch}ì—ì„œ ì¤‘ë‹¨")
                break


def main():
    parser = argparse.ArgumentParser(description='TensorFlow GPU ë©”ëª¨ë¦¬ ì§‘ì•½ì  í…ŒìŠ¤íŠ¸')
    parser.add_argument('--duration', type=int, default=180, help='í…ŒìŠ¤íŠ¸ ì§€ì† ì‹œê°„ (ì´ˆ)')
    parser.add_argument('--num-tensors', type=int, default=30, help='í• ë‹¹í•  GPU í…ì„œ ìˆ˜')
    parser.add_argument('--tensor-size', type=int, default=3000, help='í…ì„œ í¬ê¸°')
    parser.add_argument('--stress-level', type=int, default=2, help='ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ë ˆë²¨')
    parser.add_argument('--skip-progressive', action='store_true', help='ì ì§„ì  í• ë‹¹ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--skip-stress', action='store_true', help='ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--skip-training', action='store_true', help='í•™ìŠµ í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°')
    
    args = parser.parse_args()
    
    # TensorFlow GPU ì„¤ì •
    gpu_available = setup_tensorflow_gpu()
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    print_system_info()
    
    # TensorFlow ì •ë³´
    print(f"\nTensorFlow ë²„ì „: {tf.__version__}")
    if gpu_available:
        gpus = tf.config.list_physical_devices('GPU')
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ GPU: {len(gpus)}")
        for i, gpu in enumerate(gpus):
            print(f"  GPU {i}: {gpu.name}")
    
    # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘
    monitor = ResourceMonitor(interval=0.5)
    monitor.start_monitoring()
    
    print(f"\nğŸ¯ TensorFlow GPU ë©”ëª¨ë¦¬ ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ ì‹œì‘ (ì§€ì† ì‹œê°„: {args.duration}ì´ˆ)")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        
        # 1. ëŒ€ëŸ‰ GPU í…ì„œ í• ë‹¹
        print("\n1. ëŒ€ëŸ‰ GPU í…ì„œ í• ë‹¹")
        gpu_tensors = allocate_large_gpu_tensors(
            num_tensors=args.num_tensors,
            tensor_shape=(args.tensor_size, args.tensor_size)
        )
        
        # 2. ëŒ€ìš©ëŸ‰ GPU ëª¨ë¸ ìƒì„±
        print("\n2. ëŒ€ìš©ëŸ‰ GPU ëª¨ë¸ ìƒì„±")
        gpu_model = create_massive_gpu_model()
        
        # 3. ì ì§„ì  GPU ë©”ëª¨ë¦¬ í• ë‹¹
        if not args.skip_progressive:
            print("\n3. ì ì§„ì  GPU ë©”ëª¨ë¦¬ í• ë‹¹")
            progressive_tensors = progressive_gpu_memory_allocation(max_iterations=50)
        
        # 4. GPU ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
        if not args.skip_stress:
            print("\n4. GPU ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸")
            stress_tensors = gpu_memory_stress_test(stress_level=args.stress_level)
        
        # 5. GPU í•™ìŠµ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸
        if not args.skip_training:
            print("\n5. GPU í•™ìŠµ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸")
            gpu_training_memory_test(batch_size=64, num_epochs=15)
        
        # ë‚¨ì€ ì‹œê°„ ë™ì•ˆ ì¶”ê°€ GPU í…ì„œ í• ë‹¹
        elapsed_time = time.time() - start_time
        remaining_time = args.duration - elapsed_time
        
        if remaining_time > 15:
            print(f"\n6. ì¶”ê°€ GPU í…ì„œ í• ë‹¹ ({remaining_time:.1f}ì´ˆ ë™ì•ˆ)")
            extra_tensors = allocate_large_gpu_tensors(
                num_tensors=max(5, int(remaining_time / 10)),
                tensor_shape=(2000, 2000)
            )
        
    except KeyboardInterrupt:
        print("\ní…ŒìŠ¤íŠ¸ê°€ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\ní…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
        monitor.stop_monitoring()
        monitor.print_summary()
        
        # ìµœì¢… ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸
        final_usage = monitor.get_current_usage()
        print(f"\nìµœì¢… ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰:")
        if gpu_available:
            for i in range(len(tf.config.list_physical_devices('GPU'))):
                gpu_memory = final_usage.get(f'gpu_{i}_memory_used', 0)
                print(f"GPU {i} ë©”ëª¨ë¦¬: {gpu_memory:.1f} MB")
        
        print("\nğŸ¯ TensorFlow GPU ë©”ëª¨ë¦¬ ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("=" * 60)


if __name__ == "__main__":
    main() 