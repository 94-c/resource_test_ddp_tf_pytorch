"""
TensorFlow GPU ì‚¬ìš©ë¥  ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ (DCGM ë©”íŠ¸ë¦­ ëŒ€ì‘)
ì´ í…ŒìŠ¤íŠ¸ëŠ” GPU ì‚¬ìš©ë¥ ì„ ìµœëŒ€í™”í•˜ì—¬ DCGM_FI_PROF_GR_ENGINE_ACTIVE ë©”íŠ¸ë¦­ì— ë‚˜íƒ€ë‚˜ë„ë¡ í•©ë‹ˆë‹¤.
"""

import tensorflow as tf
import numpy as np
import time
import argparse
import sys
import os
import threading
from concurrent.futures import ThreadPoolExecutor
import signal

# Add utils directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.resource_monitor import ResourceMonitor, print_system_info


class IntensiveTensorFlowGPUWorkload:
    """DCGM ë©”íŠ¸ë¦­ì— ë‚˜íƒ€ë‚˜ë„ë¡ í•˜ëŠ” ì§‘ì•½ì  TensorFlow GPU ì›Œí¬ë¡œë“œ"""
    
    def __init__(self, duration_minutes=10):
        self.duration_minutes = duration_minutes
        self.duration_seconds = duration_minutes * 60
        self.stop_event = threading.Event()
        self.workload_threads = []
        
        # GPU ì„¤ì •
        self.gpus = tf.config.list_physical_devices('GPU')
        if self.gpus:
            try:
                for gpu in self.gpus:
                    tf.config.experimental.set_memory_growth(gpu, True)
                print(f"âœ… TensorFlow GPU ì„¤ì • ì™„ë£Œ: {len(self.gpus)}ê°œ GPU ì‚¬ìš© ê°€ëŠ¥")
                for i, gpu in enumerate(self.gpus):
                    print(f"  GPU {i}: {gpu.name}")
            except RuntimeError as e:
                print(f"âŒ GPU ì„¤ì • ì˜¤ë¥˜: {e}")
                self.gpus = []
        else:
            print("âš ï¸  GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    
    def create_intensive_model(self, device_name):
        """GPU ì§‘ì•½ì  ëª¨ë¸ ìƒì„±"""
        with tf.device(device_name):
            # ë§¤ìš° í° ëª¨ë¸ë¡œ GPU ì‚¬ìš©ë¥  ìµœëŒ€í™”
            model = tf.keras.Sequential([
                # ëŒ€ìš©ëŸ‰ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ë“¤
                tf.keras.layers.Conv2D(128, (7, 7), activation='relu', input_shape=(512, 512, 3), padding='same'),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.Conv2D(256, (5, 5), activation='relu', padding='same'),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.MaxPooling2D((2, 2)),
                
                tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same'),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.Conv2D(1024, (3, 3), activation='relu', padding='same'),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.MaxPooling2D((2, 2)),
                
                tf.keras.layers.Conv2D(2048, (3, 3), activation='relu', padding='same'),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.Conv2D(1024, (3, 3), activation='relu', padding='same'),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.MaxPooling2D((2, 2)),
                
                # ì—…ìƒ˜í”Œë§ ë ˆì´ì–´ë“¤
                tf.keras.layers.UpSampling2D((2, 2)),
                tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same'),
                tf.keras.layers.BatchNormalization(),
                
                tf.keras.layers.UpSampling2D((2, 2)),
                tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),
                tf.keras.layers.BatchNormalization(),
                
                tf.keras.layers.UpSampling2D((2, 2)),
                tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
                tf.keras.layers.BatchNormalization(),
                
                # ê¸€ë¡œë²Œ í‰ê·  í’€ë§ ë° Dense ë ˆì´ì–´ë“¤
                tf.keras.layers.GlobalAveragePooling2D(),
                tf.keras.layers.Dense(2048, activation='relu'),
                tf.keras.layers.Dropout(0.5),
                tf.keras.layers.Dense(4096, activation='relu'),
                tf.keras.layers.Dropout(0.5),
                tf.keras.layers.Dense(2048, activation='relu'),
                tf.keras.layers.Dropout(0.5),
                tf.keras.layers.Dense(1000, activation='relu'),
                tf.keras.layers.Dense(100, activation='softmax')
            ])
            
            model.compile(
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy']
            )
            
            return model
    
    def continuous_gpu_workload(self, gpu_id, workload_id):
        """ì§€ì†ì ì¸ GPU ì›Œí¬ë¡œë“œ ì‹¤í–‰"""
        print(f"ğŸš€ GPU {gpu_id} ì›Œí¬ë¡œë“œ {workload_id} ì‹œì‘")
        
        try:
            device_name = f'/GPU:{gpu_id}'
            model = self.create_intensive_model(device_name)
            
            # í° ë°°ì¹˜ í¬ê¸°ë¡œ GPU ì‚¬ìš©ë¥  ìµœëŒ€í™”
            batch_size = 16
            start_time = time.time()
            iteration = 0
            
            with tf.device(device_name):
                while not self.stop_event.is_set() and (time.time() - start_time) < self.duration_seconds:
                    try:
                        # í° ì´ë¯¸ì§€ ë°ì´í„° ìƒì„±
                        X = tf.random.normal([batch_size, 512, 512, 3])
                        y = tf.random.uniform([batch_size], 0, 100, dtype=tf.int32)
                        
                        # í•™ìŠµ ìŠ¤í…
                        with tf.GradientTape() as tape:
                            predictions = model(X, training=True)
                            loss = tf.keras.losses.sparse_categorical_crossentropy(y, predictions)
                            loss = tf.reduce_mean(loss)
                        
                        gradients = tape.gradient(loss, model.trainable_variables)
                        model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))
                        
                        # ì¶”ê°€ ê³„ì‚° ì§‘ì•½ì  ì—°ì‚°ë“¤
                        if iteration % 5 == 0:
                            # ëŒ€ìš©ëŸ‰ í–‰ë ¬ ì—°ì‚°
                            matrix_a = tf.random.normal([2048, 2048])
                            matrix_b = tf.random.normal([2048, 2048])
                            result = tf.matmul(matrix_a, matrix_b)
                            
                            # ê³ ìœ ê°’ ë¶„í•´ (ë§¤ìš° ì§‘ì•½ì )
                            try:
                                eigenvalues = tf.linalg.eigvals(result[:1024, :1024])
                            except:
                                pass
                            
                            # FFT ì—°ì‚°
                            signal_data = tf.random.normal([8192])
                            fft_result = tf.signal.fft(tf.cast(signal_data, tf.complex64))
                            ifft_result = tf.signal.ifft(fft_result)
                            
                            # 2D FFT
                            image_data = tf.random.normal([512, 512])
                            fft_2d = tf.signal.fft2d(tf.cast(image_data, tf.complex64))
                            ifft_2d = tf.signal.ifft2d(fft_2d)
                        
                        # ë©”ëª¨ë¦¬ ì••ë°• ë°©ì§€ë¥¼ ìœ„í•œ ì£¼ê¸°ì  ì •ë¦¬
                        if iteration % 20 == 0:
                            tf.keras.backend.clear_session()
                            elapsed = time.time() - start_time
                            remaining = self.duration_seconds - elapsed
                            print(f"  GPU {gpu_id} ì›Œí¬ë¡œë“œ {workload_id}: {iteration} ë°˜ë³µ ì™„ë£Œ, "
                                  f"ë‚¨ì€ ì‹œê°„: {remaining:.1f}ì´ˆ, Loss: {loss.numpy():.4f}")
                        
                        iteration += 1
                        
                    except tf.errors.ResourceExhaustedError:
                        print(f"  GPU {gpu_id} ë©”ëª¨ë¦¬ ë¶€ì¡±, ë°°ì¹˜ í¬ê¸° ì¤„ì„")
                        batch_size = max(1, batch_size // 2)
                        tf.keras.backend.clear_session()
                        model = self.create_intensive_model(device_name)
                        continue
                    except Exception as e:
                        print(f"  GPU {gpu_id} ì›Œí¬ë¡œë“œ ì˜¤ë¥˜: {e}")
                        continue
            
            print(f"âœ… GPU {gpu_id} ì›Œí¬ë¡œë“œ {workload_id} ì™„ë£Œ ({iteration} ë°˜ë³µ)")
            
        except Exception as e:
            print(f"âŒ GPU {gpu_id} ì›Œí¬ë¡œë“œ {workload_id} ì‹¤íŒ¨: {e}")
    
    def start_workloads(self):
        """ëª¨ë“  GPUì—ì„œ ì›Œí¬ë¡œë“œ ì‹œì‘"""
        if not self.gpus:
            print("GPUê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
            return
        
        print(f"ğŸ¯ {len(self.gpus)}ê°œ GPUì—ì„œ {self.duration_minutes}ë¶„ ë™ì•ˆ ì§‘ì•½ì  ì›Œí¬ë¡œë“œ ì‹œì‘")
        print("   DCGM_FI_PROF_GR_ENGINE_ACTIVE ë©”íŠ¸ë¦­ì— ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
        
        # ê° GPUì—ì„œ ì—¬ëŸ¬ ì›Œí¬ë¡œë“œ ì‹¤í–‰
        for gpu_id in range(len(self.gpus)):
            # GPUë‹¹ 2ê°œì˜ ì›Œí¬ë¡œë“œ ìŠ¤ë ˆë“œ ì‹¤í–‰
            for workload_id in range(2):
                thread = threading.Thread(
                    target=self.continuous_gpu_workload,
                    args=(gpu_id, workload_id)
                )
                thread.daemon = True
                thread.start()
                self.workload_threads.append(thread)
        
        # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ë“±ë¡
        def signal_handler(signum, frame):
            print(f"\nğŸ›‘ ì¤‘ë‹¨ ì‹ í˜¸ ë°›ìŒ. ì›Œí¬ë¡œë“œ ì¤‘ì§€ ì¤‘...")
            self.stop_event.set()
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
        # ì›Œí¬ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°
        start_time = time.time()
        try:
            while (time.time() - start_time) < self.duration_seconds and not self.stop_event.is_set():
                time.sleep(10)
                elapsed = time.time() - start_time
                remaining = self.duration_seconds - elapsed
                print(f"â±ï¸  ì§„í–‰ ì¤‘... ë‚¨ì€ ì‹œê°„: {remaining:.1f}ì´ˆ")
        except KeyboardInterrupt:
            print(f"\nğŸ›‘ ì‚¬ìš©ì ì¤‘ë‹¨ ìš”ì²­")
            self.stop_event.set()
        
        # ëª¨ë“  ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
        self.stop_event.set()
        for thread in self.workload_threads:
            thread.join(timeout=5)
        
        print(f"ğŸ ëª¨ë“  GPU ì›Œí¬ë¡œë“œ ì™„ë£Œ")


def main():
    parser = argparse.ArgumentParser(description='TensorFlow GPU ì‚¬ìš©ë¥  ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ (DCGM ë©”íŠ¸ë¦­ ëŒ€ì‘)')
    parser.add_argument('--duration', type=int, default=600, help='í…ŒìŠ¤íŠ¸ ì§€ì† ì‹œê°„ (ì´ˆ)')
    
    args = parser.parse_args()
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    print_system_info()
    
    # GPU ì •ë³´ ì¶œë ¥
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        print(f"\nğŸ® TensorFlow GPU ì •ë³´:")
        for i, gpu in enumerate(gpus):
            print(f"  GPU {i}: {gpu.name}")
    
    # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘
    monitor = ResourceMonitor(interval=1)
    monitor.start_monitoring()
    
    duration_minutes = max(1, args.duration // 60)
    print(f"\nğŸš€ TensorFlow GPU ì‚¬ìš©ë¥  ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ ì‹œì‘ ({duration_minutes}ë¶„)")
    print("   DCGM_FI_PROF_GR_ENGINE_ACTIVE ë©”íŠ¸ë¦­ì— ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
    print("=" * 60)
    
    # GPU ì‚¬ìš©ë¥  ìµœëŒ€í™” ê°ì²´ ìƒì„±
    gpu_workload = IntensiveTensorFlowGPUWorkload(duration_minutes=duration_minutes)
    
    start_time = time.time()
    
    try:
        # ì§‘ì•½ì  GPU ì›Œí¬ë¡œë“œ ì‹œì‘
        gpu_workload.start_workloads()
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ í…ŒìŠ¤íŠ¸ê°€ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
        monitor.stop_monitoring()
        monitor.print_summary()
        
        # ìµœì¢… ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸
        final_usage = monitor.get_current_usage()
        print(f"\nğŸ“Š ìµœì¢… ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰:")
        if gpus:
            for i in range(len(gpus)):
                gpu_utilization = final_usage.get(f'gpu_{i}_utilization', 0)
                gpu_memory = final_usage.get(f'gpu_{i}_memory_used', 0)
                print(f"  GPU {i} ì‚¬ìš©ë¥ : {gpu_utilization}%")
                print(f"  GPU {i} ë©”ëª¨ë¦¬: {gpu_memory:.2f} GB")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        print("\nğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
        tf.keras.backend.clear_session()
        
        total_duration = time.time() - start_time
        print(f"\nğŸ¯ TensorFlow GPU ì‚¬ìš©ë¥  ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"   ì´ ì‹¤í–‰ ì‹œê°„: {total_duration:.1f}ì´ˆ")
        print("=" * 60)


if __name__ == "__main__":
    main() 