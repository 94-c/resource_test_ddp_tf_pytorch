"""
PyTorch DDP ë¶„ì‚° í•™ìŠµ í…ŒìŠ¤íŠ¸
ì´ í…ŒìŠ¤íŠ¸ëŠ” ê°€ì§œ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ì‚° í•™ìŠµì„ ìˆ˜í–‰í•˜ê³  ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, Dataset
import torchvision.models as models
import numpy as np
import time
import argparse
import sys
import os
from pathlib import Path

# Add utils directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.resource_monitor import ResourceMonitor, print_system_info


class LazyFakeDataset(Dataset):
    """
    ê°€ì§œ ë°ì´í„°ì…‹ - ì‹¤ì œ ì´ë¯¸ì§€ ëŒ€ì‹  ë¬´ì‘ìœ„ í…ì„œì™€ ë¼ë²¨ì„ ìƒì„±í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì†Œí™”
    """
    def __init__(self, num_samples=100000, image_size=(3, 224, 224), num_classes=100):
        self.num_samples = num_samples
        self.image_size = image_size
        self.num_classes = num_classes
        
    def __len__(self):
        return self.num_samples
    
    def __getitem__(self, idx):
        # ë¬´ì‘ìœ„ ì´ë¯¸ì§€ ìƒì„± (3x224x224)
        image = torch.randn(self.image_size, dtype=torch.float32)
        # 0~99 ì‚¬ì´ì˜ ë¬´ì‘ìœ„ ë¼ë²¨ ìƒì„±
        label = torch.randint(0, self.num_classes, (1,)).item()
        return image, label


def setup(rank, world_size, use_cpu=False):
    """DDP í™˜ê²½ ì´ˆê¸°í™”"""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    
    # Initialize the process group
    backend = "gloo" if use_cpu else "nccl"
    dist.init_process_group(backend, rank=rank, world_size=world_size)
    
    if not use_cpu:
        torch.cuda.set_device(rank)


def cleanup():
    """DDP í™˜ê²½ ì •ë¦¬"""
    dist.destroy_process_group()


def create_model(num_classes=100):
    """ë³µì¡í•œ ResNet ëª¨ë¸ ìƒì„±"""
    model = models.resnet50(pretrained=False)
    # ì¶œë ¥ í´ë˜ìŠ¤ ìˆ˜ ì¡°ì •
    model.fc = nn.Linear(model.fc.in_features, num_classes)
    return model


def train(rank, world_size, args):
    """ë¶„ì‚° í•™ìŠµ í•¨ìˆ˜"""
    print(f"Running DDP training on rank {rank}")
    
    # CPU/GPU ì„¤ì •
    use_cpu = not torch.cuda.is_available() or args.force_cpu
    device = torch.device('cpu' if use_cpu else f'cuda:{rank}')
    
    # DDP í™˜ê²½ ì„¤ì •
    setup(rank, world_size, use_cpu)
    
    # ëª¨ë¸ ìƒì„± ë° ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    model = create_model(num_classes=args.num_classes)
    model = model.to(device)
    
    if use_cpu:
        model = DDP(model)
    else:
        model = DDP(model, device_ids=[rank])
    
    # ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ì„¤ì •
    dataset = LazyFakeDataset(
        num_samples=args.num_samples,
        image_size=(3, 224, 224),
        num_classes=args.num_classes
    )
    
    # ë¶„ì‚° ìƒ˜í”ŒëŸ¬ ì‚¬ìš©
    sampler = torch.utils.data.distributed.DistributedSampler(
        dataset, 
        num_replicas=world_size, 
        rank=rank
    )
    
    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        sampler=sampler,
        num_workers=2 if use_cpu else 4,
        pin_memory=not use_cpu
    )
    
    # ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤í•¨ìˆ˜ ì„¤ì •
    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=1e-4)
    criterion = nn.CrossEntropyLoss()
    
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
    
    # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (rank 0ì—ì„œë§Œ)
    monitor = None
    if rank == 0:
        monitor = ResourceMonitor(interval=0.5)
        monitor.start_monitoring()
        device_info = "CPU" if use_cpu else f"GPU {world_size}ê°œ"
        print(f"\nğŸ”¥ PyTorch DDP í•™ìŠµ ì‹œì‘ ({device_info} ì‚¬ìš©)")
        print(f"   ìƒ˜í”Œ ìˆ˜: {args.num_samples:,}")
        print(f"   ì—í­ ìˆ˜: {args.epochs}")
        print(f"   ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
        print(f"   í´ë˜ìŠ¤ ìˆ˜: {args.num_classes}")
        print(f"   ë””ë°”ì´ìŠ¤: {device}")
        print("=" * 60)
    
    # í•™ìŠµ ë£¨í”„
    model.train()
    for epoch in range(args.epochs):
        sampler.set_epoch(epoch)  # ì—í­ë§ˆë‹¤ ìƒ˜í”ŒëŸ¬ ì„¤ì •
        
        epoch_loss = 0.0
        num_batches = 0
        epoch_start_time = time.time()
        
        for batch_idx, (data, target) in enumerate(dataloader):
            data = data.to(device, non_blocking=not use_cpu)
            target = target.to(device)
            
            # Forward pass
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            
            # Backward pass
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            num_batches += 1
            
            # ì§„í–‰ìƒí™© ì¶œë ¥ (rank 0ì—ì„œë§Œ)
            if rank == 0 and batch_idx % 50 == 0:
                print(f"  Epoch {epoch+1}/{args.epochs}, Batch {batch_idx}/{len(dataloader)}, "
                      f"Loss: {loss.item():.4f}")
        
        # ì—í­ ì™„ë£Œ í›„ ì •ë³´ ì¶œë ¥
        epoch_time = time.time() - epoch_start_time
        avg_loss = epoch_loss / num_batches
        
        if rank == 0:
            print(f"âœ… Epoch {epoch+1}/{args.epochs} ì™„ë£Œ - "
                  f"í‰ê·  Loss: {avg_loss:.4f}, ì‹œê°„: {epoch_time:.2f}ì´ˆ")
        
        # í•™ìŠµë¥  ì—…ë°ì´íŠ¸
        scheduler.step()
        
        # ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (rank 0ì—ì„œë§Œ, 10 ì—í­ë§ˆë‹¤)
        if rank == 0 and (epoch + 1) % 10 == 0:
            save_checkpoint(model, optimizer, epoch + 1, args.save_dir, f"checkpoint_epoch_{epoch+1}.pth")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥ (rank 0ì—ì„œë§Œ)
    if rank == 0:
        save_model(model, args.save_dir, "model_final.pth")
        print(f"\nğŸ¯ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {args.save_dir}/model_final.pth")
        
        # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
        if monitor:
            monitor.stop_monitoring()
            monitor.print_summary()
            
            # ìµœì¢… ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì¶œë ¥
            final_usage = monitor.get_current_usage()
            print(f"\nìµœì¢… ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰:")
            print(f"CPU: {final_usage.get('cpu_cores', 0):.2f} cores")
            print(f"Memory: {final_usage.get('memory_gb', 0):.2f} GB")
            
            # GPU ì‚¬ìš©ëŸ‰ ì¶œë ¥ (GPU ì‚¬ìš© ì‹œë§Œ)
            if not use_cpu:
                for i in range(world_size):
                    gpu_util = final_usage.get(f'gpu_{i}_utilization', 0)
                    gpu_mem = final_usage.get(f'gpu_{i}_memory_used_gb', 0)
                    if gpu_util > 0 or gpu_mem > 0:
                        print(f"GPU {i}: {gpu_util:.1f}% ì‚¬ìš©ë¥ , {gpu_mem:.2f} GB ë©”ëª¨ë¦¬")
    
    cleanup()


def save_model(model, save_dir, filename):
    """ëª¨ë¸ ì €ì¥"""
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # DDP ëª¨ë¸ì˜ ê²½ìš° moduleì„ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ ëª¨ë¸ ì €ì¥
    if hasattr(model, 'module'):
        model_state = model.module.state_dict()
    else:
        model_state = model.state_dict()
    
    torch.save(model_state, save_dir / filename)
    print(f"ëª¨ë¸ ì €ì¥: {save_dir / filename}")


def save_checkpoint(model, optimizer, epoch, save_dir, filename):
    """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # DDP ëª¨ë¸ì˜ ê²½ìš° moduleì„ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ ëª¨ë¸ ì €ì¥
    if hasattr(model, 'module'):
        model_state = model.module.state_dict()
    else:
        model_state = model.state_dict()
    
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model_state,
        'optimizer_state_dict': optimizer.state_dict(),
        'timestamp': time.time()
    }
    
    torch.save(checkpoint, save_dir / filename)
    print(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {save_dir / filename}")


def main():
    parser = argparse.ArgumentParser(description='PyTorch DDP ë¶„ì‚° í•™ìŠµ í…ŒìŠ¤íŠ¸')
    parser.add_argument('--num-samples', type=int, default=100000, help='ë°ì´í„°ì…‹ ìƒ˜í”Œ ìˆ˜')
    parser.add_argument('--epochs', type=int, default=100, help='í•™ìŠµ ì—í­ ìˆ˜')
    parser.add_argument('--batch-size', type=int, default=32, help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--lr', type=float, default=0.01, help='í•™ìŠµë¥ ')
    parser.add_argument('--num-classes', type=int, default=100, help='í´ë˜ìŠ¤ ìˆ˜')
    parser.add_argument('--save-dir', type=str, default='./saved_models', help='ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--force-single-gpu', action='store_true', help='ë‹¨ì¼ GPU ê°•ì œ ì‚¬ìš©')
    parser.add_argument('--force-cpu', action='store_true', help='CPU ê°•ì œ ì‚¬ìš©')
    
    args = parser.parse_args()
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    print_system_info()
    
    # CPU/GPU ì„¤ì •
    if args.force_cpu:
        print("âš ï¸  CPU ëª¨ë“œë¡œ ê°•ì œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        world_size = 1
    elif not torch.cuda.is_available():
        print("âš ï¸  CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        world_size = 1
        args.force_cpu = True
    else:
        world_size = torch.cuda.device_count()
        print(f"ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ GPU ê°œìˆ˜: {world_size}")
        
        # ë¶„ì‚° í•™ìŠµ ì‹¤í–‰ ì¡°ê±´ í™•ì¸
        if world_size < 2 and not args.force_single_gpu:
            print("âŒ ë¶„ì‚° í•™ìŠµì„ ìœ„í•´ì„œëŠ” 2ê°œ ì´ìƒì˜ GPUê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            print("   ë‹¨ì¼ GPUì—ì„œ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´ --force-single-gpu ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            print("   CPUì—ì„œ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´ --force-cpu ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            return
        elif args.force_single_gpu:
            world_size = 1
            print("âš ï¸  ë‹¨ì¼ GPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    
    device_info = "CPU" if args.force_cpu else f"{world_size}ê°œ GPU"
    print(f"\nğŸš€ {device_info}ë¡œ DDP í•™ìŠµ ì‹œì‘")
    print("=" * 60)
    
    try:
        if world_size == 1:
            # ë‹¨ì¼ ë””ë°”ì´ìŠ¤ í•™ìŠµ
            train(0, 1, args)
        else:
            # ë©€í‹° GPU ë¶„ì‚° í•™ìŠµ
            mp.spawn(train, args=(world_size, args), nprocs=world_size, join=True)
            
        print("\nâœ… DDP í•™ìŠµì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        print(f"\nâŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    print("=" * 60)


if __name__ == "__main__":
    main() 