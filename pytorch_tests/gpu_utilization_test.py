"""
PyTorch GPU ì‚¬ìš©ë¥  ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ (DCGM ë©”íŠ¸ë¦­ ëŒ€ì‘)
ì´ í…ŒìŠ¤íŠ¸ëŠ” GPU ì‚¬ìš©ë¥ ì„ ìµœëŒ€í™”í•˜ì—¬ DCGM_FI_PROF_GR_ENGINE_ACTIVE ë©”íŠ¸ë¦­ì— ë‚˜íƒ€ë‚˜ë„ë¡ í•©ë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import time
import argparse
import sys
import os
import threading
import concurrent.futures
from typing import List, Dict, Optional
import signal
import multiprocessing as mp

# Add utils directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.resource_monitor import ResourceMonitor, print_system_info


class IntensiveGPUWorkload:
    """DCGM ë©”íŠ¸ë¦­ì— ë‚˜íƒ€ë‚˜ë„ë¡ í•˜ëŠ” ì§‘ì•½ì  GPU ì›Œí¬ë¡œë“œ"""
    
    def __init__(self, duration_minutes=10):
        self.duration_minutes = duration_minutes
        self.duration_seconds = duration_minutes * 60
        self.stop_event = threading.Event()
        self.workload_threads = []
        
        # ì•ˆì „í•œ GPU ê°ì§€
        self.device_count = 0
        self.available_devices = []
        self.cuda_available = False
        
        # CUDA ê¸°ë³¸ ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸
        try:
            if not torch.cuda.is_available():
                print("âš ï¸  CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
                return
                
            # PyTorch CUDA ì´ˆê¸°í™” ì•ˆì „ì„± í™•ì¸
            try:
                torch.cuda.init()
                potential_devices = torch.cuda.device_count()
                print(f"ğŸ” ê°ì§€ëœ GPU ê°œìˆ˜: {potential_devices}")
            except Exception as e:
                print(f"âŒ CUDA ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                print("âš ï¸  CPU ëª¨ë“œë¡œ ì „í™˜ë©ë‹ˆë‹¤.")
                return
                
        except Exception as e:
            print(f"âŒ CUDA í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            print("âš ï¸  CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
            return
        
        # ì•ˆì „í•œ GPU ì¥ì¹˜ ê°ì§€
        for i in range(potential_devices):
            try:
                print(f"ğŸ” GPU {i} ì ‘ê·¼ì„± í…ŒìŠ¤íŠ¸ ì¤‘...")
                
                # ë‹¨ê³„ë³„ ì•ˆì „í•œ GPU ì ‘ê·¼ í…ŒìŠ¤íŠ¸
                # 1ë‹¨ê³„: ì¥ì¹˜ ì†ì„± í™•ì¸
                try:
                    props = torch.cuda.get_device_properties(i)
                    device_name = props.name
                    total_memory = props.total_memory
                except Exception as e:
                    print(f"âŒ GPU {i}: ì¥ì¹˜ ì†ì„± ì¡°íšŒ ì‹¤íŒ¨ - {e}")
                    continue
                
                # 2ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ ìƒì„± ë° ë©”ëª¨ë¦¬ í• ë‹¹ í…ŒìŠ¤íŠ¸
                try:
                    with torch.cuda.device(i):
                        # ë§¤ìš° ì‘ì€ ë©”ëª¨ë¦¬ í• ë‹¹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
                        test_tensor = torch.zeros(1, device=f'cuda:{i}', dtype=torch.float32)
                        
                        # ê°„ë‹¨í•œ ì—°ì‚° í…ŒìŠ¤íŠ¸
                        result = test_tensor + 1.0
                        
                        # ë™ê¸°í™” í…ŒìŠ¤íŠ¸ (ì´ ë¶€ë¶„ì—ì„œ ë§ì€ ì˜¤ë¥˜ ë°œìƒ)
                        torch.cuda.synchronize(device=i)
                        
                        # ë©”ëª¨ë¦¬ ì •ë¦¬
                        del test_tensor, result
                        torch.cuda.empty_cache()
                        
                except Exception as e:
                    print(f"âŒ GPU {i}: ë©”ëª¨ë¦¬/ì—°ì‚° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - {str(e)[:100]}...")
                    if "device >= 0 && device < num_gpus" in str(e):
                        print(f"  â†’ MIG í™˜ê²½ ë˜ëŠ” GPU ë§¤í•‘ ë¬¸ì œ")
                    elif "CUDA out of memory" in str(e):
                        print(f"  â†’ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±")
                    elif "no kernel image" in str(e):
                        print(f"  â†’ CUDA ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œ")
                    continue
                
                # ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼ ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ì¥ì¹˜ë¡œ ì¶”ê°€
                self.available_devices.append(i)
                print(f"âœ… GPU {i}: ì ‘ê·¼ ê°€ëŠ¥")
                
                # GPU ì •ë³´ ì¶œë ¥ (ì•ˆì „í•œ ë°©ì‹)
                try:
                    print(f"  ì´ë¦„: {device_name}")
                    print(f"  ì´ ë©”ëª¨ë¦¬: {total_memory / 1024**3:.1f} GB")
                    print(f"  ë©€í‹°í”„ë¡œì„¸ì„œ: {props.multi_processor_count}")
                    
                    # í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ì˜¤ë¥˜ ë°œìƒ ì‹œ ê±´ë„ˆë›°ê¸°)
                    try:
                        allocated = torch.cuda.memory_allocated(i) / 1024**3
                        cached = torch.cuda.memory_reserved(i) / 1024**3
                        print(f"  í• ë‹¹ëœ ë©”ëª¨ë¦¬: {allocated:.2f} GB, ìºì‹œëœ ë©”ëª¨ë¦¬: {cached:.2f} GB")
                    except:
                        pass
                        
                except Exception as e:
                    print(f"  ì •ë³´ ì¶œë ¥ ì¤‘ ì˜¤ë¥˜ (í•˜ì§€ë§Œ GPUëŠ” ì‚¬ìš© ê°€ëŠ¥): {e}")
                    
            except Exception as e:
                print(f"âŒ GPU {i}: ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ - {str(e)[:100]}...")
                continue
        
        self.device_count = len(self.available_devices)
        
        if self.device_count == 0:
            print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ GPUê°€ ì—†ìŠµë‹ˆë‹¤.")
            self.cuda_available = False
        else:
            print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ GPU: {self.device_count}ê°œ (ì¸ë±ìŠ¤: {self.available_devices})")
            self.cuda_available = True
    
    def create_intensive_model(self, device_id):
        """GPU ì§‘ì•½ì  ëª¨ë¸ ìƒì„± (ì»¨í…Œì´ë„ˆ í™˜ê²½ ìµœì í™”)"""
        class SafeIntensiveModel(nn.Module):
            def __init__(self):
                super(SafeIntensiveModel, self).__init__()
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì¸ ì•ˆì „í•œ ëª¨ë¸
                self.conv_layers = nn.ModuleList([
                    nn.Conv2d(3, 64, kernel_size=5, padding=2),
                    nn.Conv2d(64, 128, kernel_size=3, padding=1),
                    nn.Conv2d(128, 256, kernel_size=3, padding=1),
                    nn.Conv2d(256, 512, kernel_size=3, padding=1),
                    nn.Conv2d(512, 256, kernel_size=3, padding=1),
                    nn.Conv2d(256, 128, kernel_size=3, padding=1),
                    nn.Conv2d(128, 64, kernel_size=3, padding=1),
                    nn.Conv2d(64, 32, kernel_size=3, padding=1),
                ])
                
                self.batch_norms = nn.ModuleList([
                    nn.BatchNorm2d(64),
                    nn.BatchNorm2d(128),
                    nn.BatchNorm2d(256),
                    nn.BatchNorm2d(512),
                    nn.BatchNorm2d(256),
                    nn.BatchNorm2d(128),
                    nn.BatchNorm2d(64),
                    nn.BatchNorm2d(32),
                ])
                
                # ë§ˆì§€ë§‰ ì»¨ë³¼ë£¨ì…˜ ì¶œë ¥ ê³„ì‚°: 32 channels Ã— 4 Ã— 4 = 512
                self.final_conv = nn.Conv2d(32, 32, kernel_size=3, padding=1)
                
                # ì°¨ì› ë§ì¶˜ Dense ë ˆì´ì–´ë“¤ (32 Ã— 4 Ã— 4 = 512)
                self.dense_layers = nn.ModuleList([
                    nn.Linear(512, 256),
                    nn.Linear(256, 128),
                    nn.Linear(128, 100)
                ])
                
            def forward(self, x):
                # ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ë“¤
                for i, (conv, bn) in enumerate(zip(self.conv_layers, self.batch_norms)):
                    x = conv(x)
                    x = F.relu(x)
                    x = bn(x)
                    
                    # ì ì ˆí•œ ë‹¤ìš´ìƒ˜í”Œë§
                    if i in [1, 3]:
                        x = F.max_pool2d(x, 2)
                    elif i in [5, 7]:
                        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)
                
                # ë§ˆì§€ë§‰ ì»¨ë³¼ë£¨ì…˜
                x = self.final_conv(x)
                x = F.relu(x)
                
                # ê¸€ë¡œë²Œ í‰ê·  í’€ë§ (32 Ã— 4 Ã— 4 = 512)
                x = F.adaptive_avg_pool2d(x, (4, 4))
                x = x.view(x.size(0), -1)  # (batch_size, 512)
                
                # Dense ë ˆì´ì–´ë“¤
                for dense in self.dense_layers:
                    x = F.relu(dense(x))
                
                return x
        
        device = torch.device(f'cuda:{device_id}')
        model = SafeIntensiveModel().to(device)
        return model, device
    
    def continuous_gpu_workload(self, device_id, workload_id):
        """ì§€ì†ì ì¸ GPU ì›Œí¬ë¡œë“œ ì‹¤í–‰ (ì•ˆì „ ë²„ì „)"""
        print(f"ğŸš€ GPU {device_id} ì›Œí¬ë¡œë“œ {workload_id} ì‹œì‘")
        
        try:
            torch.cuda.set_device(device_id)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.cuda.empty_cache()
            
            model, device = self.create_intensive_model(device_id)
            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
            criterion = nn.CrossEntropyLoss()
            
            # ë” ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¡œ ì‹œì‘
            batch_size = 8
            start_time = time.time()
            iteration = 0
            
            while not self.stop_event.is_set() and (time.time() - start_time) < self.duration_seconds:
                try:
                    # ë” ì‘ì€ ì´ë¯¸ì§€ ë°ì´í„° ìƒì„±
                    input_data = torch.randn(batch_size, 3, 256, 256, device=device)
                    target = torch.randint(0, 100, (batch_size,), device=device)
                    
                    # ìˆœì „íŒŒ
                    optimizer.zero_grad()
                    output = model(input_data)
                    loss = criterion(output, target)
                    
                    # ì—­ì „íŒŒ
                    loss.backward()
                    optimizer.step()
                    
                    # ë” ì§‘ì•½ì ì´ì§€ë§Œ ì•ˆì „í•œ ì—°ì‚°ë“¤
                    if iteration % 3 == 0:
                        # ì¤‘ê°„ í¬ê¸° í–‰ë ¬ ì—°ì‚°
                        matrix_a = torch.randn(1024, 1024, device=device)
                        matrix_b = torch.randn(1024, 1024, device=device)
                        result = torch.matmul(matrix_a, matrix_b)
                        
                        # ê°„ë‹¨í•œ FFT ì—°ì‚°
                        fft_input = torch.randn(4096, device=device)
                        fft_result = torch.fft.fft(fft_input)
                        ifft_result = torch.fft.ifft(fft_result)
                    
                    # ë” ë¹ˆë²ˆí•œ ë©”ëª¨ë¦¬ ì •ë¦¬ (5íšŒë§ˆë‹¤)
                    if iteration % 5 == 0:
                        torch.cuda.empty_cache()
                        elapsed = time.time() - start_time
                        remaining = self.duration_seconds - elapsed
                        
                        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
                        try:
                            allocated = torch.cuda.memory_allocated(device_id) / 1024**3
                            reserved = torch.cuda.memory_reserved(device_id) / 1024**3
                            print(f"  GPU {device_id} ì›Œí¬ë¡œë“œ {workload_id}: {iteration} ë°˜ë³µ ì™„ë£Œ, "
                                  f"ë‚¨ì€ ì‹œê°„: {remaining:.1f}ì´ˆ, Loss: {loss.item():.4f}, "
                                  f"ë©”ëª¨ë¦¬: {allocated:.2f}GB/{reserved:.2f}GB")
                        except Exception:
                            print(f"  GPU {device_id} ì›Œí¬ë¡œë“œ {workload_id}: {iteration} ë°˜ë³µ ì™„ë£Œ, "
                                  f"ë‚¨ì€ ì‹œê°„: {remaining:.1f}ì´ˆ, Loss: {loss.item():.4f}")
                    
                    iteration += 1
                    
                except torch.cuda.OutOfMemoryError as e:
                    print(f"  GPU {device_id} ë©”ëª¨ë¦¬ ë¶€ì¡±, ë°°ì¹˜ í¬ê¸° ì¤„ì„: {batch_size} -> {max(1, batch_size // 2)}")
                    batch_size = max(1, batch_size // 2)
                    torch.cuda.empty_cache()
                    time.sleep(1)  # ì ì‹œ ëŒ€ê¸°
                    continue
                except RuntimeError as e:
                    if "NVML" in str(e) or "CUDA" in str(e):
                        print(f"  GPU {device_id} CUDA/NVML ì˜¤ë¥˜ ë°œìƒ: {e}")
                        print(f"  ì›Œí¬ë¡œë“œë¥¼ ì•ˆì „í•˜ê²Œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                        break
                    else:
                        print(f"  GPU {device_id} ëŸ°íƒ€ì„ ì˜¤ë¥˜: {e}")
                        continue
                except Exception as e:
                    print(f"  GPU {device_id} ì›Œí¬ë¡œë“œ ì˜¤ë¥˜: {e}")
                    continue
            
            print(f"âœ… GPU {device_id} ì›Œí¬ë¡œë“œ {workload_id} ì™„ë£Œ ({iteration} ë°˜ë³µ)")
            
        except Exception as e:
            print(f"âŒ GPU {device_id} ì›Œí¬ë¡œë“œ {workload_id} ì‹¤íŒ¨: {e}")
        finally:
            # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
            try:
                torch.cuda.empty_cache()
            except Exception:
                pass
    
    def start_workloads(self):
        """ëª¨ë“  GPUì—ì„œ ì›Œí¬ë¡œë“œ ì‹œì‘ (ì•ˆì „ ë²„ì „)"""
        if not self.cuda_available or self.device_count == 0:
            print("GPUê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
            return
        
        print(f"ğŸ¯ {self.device_count}ê°œ GPUì—ì„œ {self.duration_minutes}ë¶„ ë™ì•ˆ ì§‘ì•½ì  ì›Œí¬ë¡œë“œ ì‹œì‘")
        print("   DCGM_FI_PROF_GR_ENGINE_ACTIVE ë©”íŠ¸ë¦­ì— ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
        print("   âš ï¸  ì»¨í…Œì´ë„ˆ í™˜ê²½ì—ì„œ ì•ˆì „ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        
        # ğŸ” ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
        print(f"\nğŸ” GPU ê°ì§€ ê²°ê³¼:")
        try:
            total_detected = torch.cuda.device_count()
            print(f"  ì „ì²´ ê°ì§€ëœ GPU: {total_detected}")
        except Exception as e:
            print(f"  ì „ì²´ ê°ì§€ëœ GPU: í™•ì¸ ë¶ˆê°€ ({e})")
        
        print(f"  ì‚¬ìš© ê°€ëŠ¥í•œ GPU ê°œìˆ˜: {self.device_count}")
        print(f"  ì‚¬ìš© ê°€ëŠ¥í•œ GPU ì¸ë±ìŠ¤: {self.available_devices}")
        
        # ê° ì‚¬ìš© ê°€ëŠ¥í•œ GPUì—ì„œ ë‹¨ì¼ ì›Œí¬ë¡œë“œë§Œ ì‹¤í–‰ (ì•ˆì „ì„± í–¥ìƒ)
        for device_id in self.available_devices:
            print(f"\nğŸš€ GPU {device_id}ì—ì„œ ì›Œí¬ë¡œë“œ ì‹œì‘...")
            
            # MIG í™˜ê²½ì—ì„œ ë” ì§‘ì•½ì ì¸ ì‚¬ìš©ì„ ìœ„í•´ GPUë‹¹ 2ê°œ ì›Œí¬ë¡œë“œ ì‹¤í–‰
            for workload_id in range(2):
                thread = threading.Thread(
                    target=self.continuous_gpu_workload,
                    args=(device_id, workload_id)
                )
                thread.daemon = True
                thread.start()
                self.workload_threads.append(thread)
                print(f"  ì›Œí¬ë¡œë“œ {workload_id} ì‹œì‘ë¨")
                
                # ì›Œí¬ë¡œë“œ ê°„ ì‹œì‘ ê°„ê²© (ë©”ëª¨ë¦¬ ê²½í•© ë°©ì§€)
                time.sleep(1)
            
            # GPU ê°„ ì‹œì‘ ê°„ê²© (ë¦¬ì†ŒìŠ¤ ê²½í•© ë°©ì§€)
            time.sleep(2)
        
        print(f"\nâœ… ì´ {len(self.workload_threads)}ê°œ ì›Œí¬ë¡œë“œ ìŠ¤ë ˆë“œ ì‹œì‘ë¨")
        
        # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ë“±ë¡
        def signal_handler(signum, frame):
            print(f"\nğŸ›‘ ì¤‘ë‹¨ ì‹ í˜¸ ë°›ìŒ. ì›Œí¬ë¡œë“œ ì¤‘ì§€ ì¤‘...")
            self.stop_event.set()
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
        # ì›Œí¬ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°
        start_time = time.time()
        try:
            while (time.time() - start_time) < self.duration_seconds and not self.stop_event.is_set():
                time.sleep(10)
                elapsed = time.time() - start_time
                remaining = self.duration_seconds - elapsed
                print(f"â±ï¸  ì§„í–‰ ì¤‘... ë‚¨ì€ ì‹œê°„: {remaining:.1f}ì´ˆ")
                
                # ì£¼ê¸°ì ìœ¼ë¡œ ì „ì²´ GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
                try:
                    for i in self.available_devices:
                        allocated = torch.cuda.memory_allocated(i) / 1024**3
                        reserved = torch.cuda.memory_reserved(i) / 1024**3
                        print(f"    GPU {i} ë©”ëª¨ë¦¬: {allocated:.2f}GB í• ë‹¹, {reserved:.2f}GB ì˜ˆì•½")
                except Exception:
                    pass
                    
        except KeyboardInterrupt:
            print(f"\nğŸ›‘ ì‚¬ìš©ì ì¤‘ë‹¨ ìš”ì²­")
            self.stop_event.set()
        
        # ëª¨ë“  ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
        self.stop_event.set()
        for thread in self.workload_threads:
            thread.join(timeout=10)
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
        try:
            for i in self.available_devices:
                torch.cuda.empty_cache()
        except Exception:
            pass
        
        print(f"ğŸ ëª¨ë“  GPU ì›Œí¬ë¡œë“œ ì™„ë£Œ")


def main():
    parser = argparse.ArgumentParser(description='PyTorch GPU ì‚¬ìš©ë¥  ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ (DCGM ë©”íŠ¸ë¦­ ëŒ€ì‘)')
    parser.add_argument('--duration', type=int, default=600, help='í…ŒìŠ¤íŠ¸ ì§€ì† ì‹œê°„ (ì´ˆ)')
    
    args = parser.parse_args()
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    print_system_info()
    
    # GPU ì •ë³´ ì¶œë ¥ (ì•ˆì „í•œ ê°ì§€)
    try:
        if torch.cuda.is_available():
            print(f"\nğŸ® GPU ì •ë³´:")
            try:
                # CUDA ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
                torch.cuda.init()
                device_count = torch.cuda.device_count()
                print(f"  ê°ì§€ëœ GPU ê°œìˆ˜: {device_count}")
                
                for i in range(device_count):
                    try:
                        # ë‹¨ê³„ì  GPU ì ‘ê·¼ í…ŒìŠ¤íŠ¸
                        props = torch.cuda.get_device_properties(i)
                        device_name = props.name
                        total_memory = props.total_memory / 1024**3
                        
                        # ë©”ëª¨ë¦¬ í• ë‹¹ í…ŒìŠ¤íŠ¸
                        with torch.cuda.device(i):
                            test_tensor = torch.zeros(1, device=f'cuda:{i}')
                            torch.cuda.synchronize(device=i)
                            del test_tensor
                        
                        print(f"  GPU {i}: {device_name}")
                        print(f"    ë©”ëª¨ë¦¬: {total_memory:.1f} GB")
                        print(f"    ë©€í‹°í”„ë¡œì„¸ì„œ: {props.multi_processor_count}")
                        
                    except Exception as e:
                        print(f"  GPU {i}: ì ‘ê·¼ ë¶ˆê°€ - {str(e)[:100]}...")
                        
            except Exception as e:
                print(f"  GPU ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        else:
            print("\nâŒ CUDA ì‚¬ìš© ë¶ˆê°€ëŠ¥")
    except Exception as e:
        print(f"\nâŒ GPU í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘
    monitor = ResourceMonitor(interval=1)
    monitor.start_monitoring()
    
    duration_minutes = max(1, args.duration // 60)
    print(f"\nğŸš€ PyTorch GPU ì‚¬ìš©ë¥  ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ ì‹œì‘ ({duration_minutes}ë¶„)")
    print("   DCGM_FI_PROF_GR_ENGINE_ACTIVE ë©”íŠ¸ë¦­ì— ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
    print("=" * 60)
    
    # GPU ì‚¬ìš©ë¥  ìµœëŒ€í™” ê°ì²´ ìƒì„±
    gpu_workload = IntensiveGPUWorkload(duration_minutes=duration_minutes)
    
    start_time = time.time()
    
    try:
        # ì§‘ì•½ì  GPU ì›Œí¬ë¡œë“œ ì‹œì‘
        gpu_workload.start_workloads()
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ í…ŒìŠ¤íŠ¸ê°€ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
        monitor.stop_monitoring()
        monitor.print_summary()
        
        # ìµœì¢… ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸
        final_usage = monitor.get_current_usage()
        print(f"\nğŸ“Š ìµœì¢… ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰:")
        
        # ì•ˆì „í•œ GPU ìƒíƒœ í™•ì¸
        try:
            if hasattr(gpu_workload, 'cuda_available') and gpu_workload.cuda_available:
                if hasattr(gpu_workload, 'available_devices') and gpu_workload.available_devices:
                    for i in gpu_workload.available_devices:
                        gpu_utilization = final_usage.get(f'gpu_{i}_utilization', 0)
                        gpu_memory = final_usage.get(f'gpu_{i}_memory_used', 0)
                        print(f"  GPU {i} ì‚¬ìš©ë¥ : {gpu_utilization}%")
                        print(f"  GPU {i} ë©”ëª¨ë¦¬: {gpu_memory:.2f} GB")
                else:
                    print("  ì‚¬ìš© ê°€ëŠ¥í•œ GPU ì—†ìŒ")
            else:
                print("  GPU ì‚¬ìš© ë¶ˆê°€ëŠ¥")
        except Exception as e:
            print(f"  GPU ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ì•ˆì „í•œ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        print("\nğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
        try:
            if hasattr(gpu_workload, 'cuda_available') and gpu_workload.cuda_available:
                if hasattr(gpu_workload, 'available_devices') and gpu_workload.available_devices:
                    # ê° ì‚¬ìš© ê°€ëŠ¥í•œ GPUì—ì„œ ê°œë³„ì ìœ¼ë¡œ ì •ë¦¬
                    for device_id in gpu_workload.available_devices:
                        try:
                            with torch.cuda.device(device_id):
                                torch.cuda.empty_cache()
                                torch.cuda.synchronize(device=device_id)
                            print(f"  GPU {device_id} ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
                        except Exception as e:
                            print(f"  GPU {device_id} ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                else:
                    print("  ì •ë¦¬í•  GPU ì—†ìŒ")
            else:
                print("  GPU ì •ë¦¬ ë¶ˆê°€ëŠ¥")
        except Exception as e:
            print(f"  GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        total_duration = time.time() - start_time
        print(f"\nğŸ¯ PyTorch GPU ì‚¬ìš©ë¥  ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"   ì´ ì‹¤í–‰ ì‹œê°„: {total_duration:.1f}ì´ˆ")
        print("=" * 60)


if __name__ == "__main__":
    main() 