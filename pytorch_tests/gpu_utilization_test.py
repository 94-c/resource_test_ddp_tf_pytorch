"""
PyTorch GPU ì‚¬ìš©ë¥  ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ (DCGM ë©”íŠ¸ë¦­ ëŒ€ì‘)
ì´ í…ŒìŠ¤íŠ¸ëŠ” GPU ì‚¬ìš©ë¥ ì„ ìµœëŒ€í™”í•˜ì—¬ DCGM_FI_PROF_GR_ENGINE_ACTIVE ë©”íŠ¸ë¦­ì— ë‚˜íƒ€ë‚˜ë„ë¡ í•©ë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import time
import argparse
import sys
import os
import threading
import concurrent.futures
from typing import List, Dict, Optional
import signal
import multiprocessing as mp

# Add utils directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.resource_monitor import ResourceMonitor, print_system_info


class IntensiveGPUWorkload:
    """DCGM ë©”íŠ¸ë¦­ì— ë‚˜íƒ€ë‚˜ë„ë¡ í•˜ëŠ” ì§‘ì•½ì  GPU ì›Œí¬ë¡œë“œ"""
    
    def __init__(self, duration_minutes=10):
        self.duration_minutes = duration_minutes
        self.duration_seconds = duration_minutes * 60
        self.device_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
        self.stop_event = threading.Event()
        self.workload_threads = []
        
        if not torch.cuda.is_available():
            print("âš ï¸  CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
            return
        
        print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {self.device_count}ê°œ GPU ê°ì§€")
        for i in range(self.device_count):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
    
    def create_intensive_model(self, device_id):
        """GPU ì§‘ì•½ì  ëª¨ë¸ ìƒì„±"""
        class IntensiveModel(nn.Module):
            def __init__(self):
                super(IntensiveModel, self).__init__()
                # ë§¤ìš° í° ëª¨ë¸ë¡œ GPU ì‚¬ìš©ë¥  ìµœëŒ€í™”
                self.conv_layers = nn.ModuleList([
                    nn.Conv2d(3, 128, kernel_size=7, padding=3),
                    nn.Conv2d(128, 256, kernel_size=5, padding=2),
                    nn.Conv2d(256, 512, kernel_size=3, padding=1),
                    nn.Conv2d(512, 1024, kernel_size=3, padding=1),
                    nn.Conv2d(1024, 2048, kernel_size=3, padding=1),
                    nn.Conv2d(2048, 1024, kernel_size=3, padding=1),
                    nn.Conv2d(1024, 512, kernel_size=3, padding=1),
                    nn.Conv2d(512, 256, kernel_size=3, padding=1),
                    nn.Conv2d(256, 128, kernel_size=3, padding=1),
                    nn.Conv2d(128, 64, kernel_size=3, padding=1),
                    nn.Conv2d(64, 32, kernel_size=3, padding=1),
                    nn.Conv2d(32, 3, kernel_size=3, padding=1)
                ])
                
                self.batch_norms = nn.ModuleList([
                    nn.BatchNorm2d(128),
                    nn.BatchNorm2d(256),
                    nn.BatchNorm2d(512),
                    nn.BatchNorm2d(1024),
                    nn.BatchNorm2d(2048),
                    nn.BatchNorm2d(1024),
                    nn.BatchNorm2d(512),
                    nn.BatchNorm2d(256),
                    nn.BatchNorm2d(128),
                    nn.BatchNorm2d(64),
                    nn.BatchNorm2d(32),
                ])
                
                # íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ ì¶”ê°€
                self.transformer_layers = nn.ModuleList([
                    nn.TransformerEncoderLayer(d_model=512, nhead=8, dim_feedforward=2048, batch_first=True)
                    for _ in range(6)
                ])
                
                # Dense ë ˆì´ì–´ë“¤
                self.dense_layers = nn.ModuleList([
                    nn.Linear(512, 2048),
                    nn.Linear(2048, 4096),
                    nn.Linear(4096, 2048),
                    nn.Linear(2048, 512),
                    nn.Linear(512, 100)
                ])
                
            def forward(self, x):
                # ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ë“¤
                for i, (conv, bn) in enumerate(zip(self.conv_layers[:-1], self.batch_norms)):
                    x = conv(x)
                    x = F.relu(x)
                    x = bn(x)
                    
                    # ë‹¤ìš´ìƒ˜í”Œë§ê³¼ ì—…ìƒ˜í”Œë§ìœ¼ë¡œ ê³„ì‚°ëŸ‰ ì¦ê°€
                    if i < 5:
                        x = F.max_pool2d(x, 2)
                    elif i >= 6:
                        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)
                
                x = self.conv_layers[-1](x)
                
                # ê¸€ë¡œë²Œ í‰ê·  í’€ë§
                x = F.adaptive_avg_pool2d(x, (16, 16))
                
                # íŠ¸ëœìŠ¤í¬ë¨¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ reshape
                batch_size = x.shape[0]
                x = x.view(batch_size, -1, 512)  # (batch, seq_len, d_model)
                
                # íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë“¤
                for transformer in self.transformer_layers:
                    x = transformer(x)
                
                # í‰ê·  í’€ë§
                x = x.mean(dim=1)  # (batch, d_model)
                
                # Dense ë ˆì´ì–´ë“¤
                for dense in self.dense_layers:
                    x = F.relu(dense(x))
                
                return x
        
        device = torch.device(f'cuda:{device_id}')
        model = IntensiveModel().to(device)
        return model, device
    
    def continuous_gpu_workload(self, device_id, workload_id):
        """ì§€ì†ì ì¸ GPU ì›Œí¬ë¡œë“œ ì‹¤í–‰"""
        print(f"ğŸš€ GPU {device_id} ì›Œí¬ë¡œë“œ {workload_id} ì‹œì‘")
        
        try:
            torch.cuda.set_device(device_id)
            model, device = self.create_intensive_model(device_id)
            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
            criterion = nn.CrossEntropyLoss()
            
            # í° ë°°ì¹˜ í¬ê¸°ë¡œ GPU ì‚¬ìš©ë¥  ìµœëŒ€í™”
            batch_size = 32
            start_time = time.time()
            iteration = 0
            
            while not self.stop_event.is_set() and (time.time() - start_time) < self.duration_seconds:
                try:
                    # í° ì´ë¯¸ì§€ ë°ì´í„° ìƒì„±
                    input_data = torch.randn(batch_size, 3, 512, 512, device=device)
                    target = torch.randint(0, 100, (batch_size,), device=device)
                    
                    # ìˆœì „íŒŒ
                    optimizer.zero_grad()
                    output = model(input_data)
                    loss = criterion(output, target)
                    
                    # ì—­ì „íŒŒ
                    loss.backward()
                    optimizer.step()
                    
                    # ì¶”ê°€ ê³„ì‚° ì§‘ì•½ì  ì—°ì‚°ë“¤
                    if iteration % 5 == 0:
                        # ëŒ€ìš©ëŸ‰ í–‰ë ¬ ì—°ì‚°
                        matrix_a = torch.randn(2048, 2048, device=device)
                        matrix_b = torch.randn(2048, 2048, device=device)
                        result = torch.matmul(matrix_a, matrix_b)
                        
                        # ê³ ìœ ê°’ ë¶„í•´ (ë§¤ìš° ì§‘ì•½ì )
                        try:
                            eigenvalues = torch.linalg.eigvals(result[:1024, :1024])
                        except:
                            pass
                        
                        # FFT ì—°ì‚°
                        fft_input = torch.randn(8192, device=device)
                        fft_result = torch.fft.fft(fft_input)
                        ifft_result = torch.fft.ifft(fft_result)
                    
                    # ë©”ëª¨ë¦¬ ì••ë°• ë°©ì§€ë¥¼ ìœ„í•œ ì£¼ê¸°ì  ì •ë¦¬
                    if iteration % 20 == 0:
                        torch.cuda.empty_cache()
                        elapsed = time.time() - start_time
                        remaining = self.duration_seconds - elapsed
                        print(f"  GPU {device_id} ì›Œí¬ë¡œë“œ {workload_id}: {iteration} ë°˜ë³µ ì™„ë£Œ, "
                              f"ë‚¨ì€ ì‹œê°„: {remaining:.1f}ì´ˆ, Loss: {loss.item():.4f}")
                    
                    iteration += 1
                    
                except torch.cuda.OutOfMemoryError:
                    print(f"  GPU {device_id} ë©”ëª¨ë¦¬ ë¶€ì¡±, ë°°ì¹˜ í¬ê¸° ì¤„ì„")
                    batch_size = max(1, batch_size // 2)
                    torch.cuda.empty_cache()
                    continue
                except Exception as e:
                    print(f"  GPU {device_id} ì›Œí¬ë¡œë“œ ì˜¤ë¥˜: {e}")
                    continue
            
            print(f"âœ… GPU {device_id} ì›Œí¬ë¡œë“œ {workload_id} ì™„ë£Œ ({iteration} ë°˜ë³µ)")
            
        except Exception as e:
            print(f"âŒ GPU {device_id} ì›Œí¬ë¡œë“œ {workload_id} ì‹¤íŒ¨: {e}")
    
    def start_workloads(self):
        """ëª¨ë“  GPUì—ì„œ ì›Œí¬ë¡œë“œ ì‹œì‘"""
        if not torch.cuda.is_available():
            print("GPUê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
            return
        
        print(f"ğŸ¯ {self.device_count}ê°œ GPUì—ì„œ {self.duration_minutes}ë¶„ ë™ì•ˆ ì§‘ì•½ì  ì›Œí¬ë¡œë“œ ì‹œì‘")
        print("   DCGM_FI_PROF_GR_ENGINE_ACTIVE ë©”íŠ¸ë¦­ì— ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
        
        # ê° GPUì—ì„œ ì—¬ëŸ¬ ì›Œí¬ë¡œë“œ ì‹¤í–‰
        for device_id in range(self.device_count):
            # GPUë‹¹ 2ê°œì˜ ì›Œí¬ë¡œë“œ ìŠ¤ë ˆë“œ ì‹¤í–‰
            for workload_id in range(2):
                thread = threading.Thread(
                    target=self.continuous_gpu_workload,
                    args=(device_id, workload_id)
                )
                thread.daemon = True
                thread.start()
                self.workload_threads.append(thread)
        
        # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ë“±ë¡
        def signal_handler(signum, frame):
            print(f"\nğŸ›‘ ì¤‘ë‹¨ ì‹ í˜¸ ë°›ìŒ. ì›Œí¬ë¡œë“œ ì¤‘ì§€ ì¤‘...")
            self.stop_event.set()
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
        # ì›Œí¬ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°
        start_time = time.time()
        try:
            while (time.time() - start_time) < self.duration_seconds and not self.stop_event.is_set():
                time.sleep(10)
                elapsed = time.time() - start_time
                remaining = self.duration_seconds - elapsed
                print(f"â±ï¸  ì§„í–‰ ì¤‘... ë‚¨ì€ ì‹œê°„: {remaining:.1f}ì´ˆ")
        except KeyboardInterrupt:
            print(f"\nğŸ›‘ ì‚¬ìš©ì ì¤‘ë‹¨ ìš”ì²­")
            self.stop_event.set()
        
        # ëª¨ë“  ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
        self.stop_event.set()
        for thread in self.workload_threads:
            thread.join(timeout=5)
        
        print(f"ï¿½ï¿½ ëª¨ë“  GPU ì›Œí¬ë¡œë“œ ì™„ë£Œ")


def main():
    parser = argparse.ArgumentParser(description='PyTorch GPU ì‚¬ìš©ë¥  ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ (DCGM ë©”íŠ¸ë¦­ ëŒ€ì‘)')
    parser.add_argument('--duration', type=int, default=600, help='í…ŒìŠ¤íŠ¸ ì§€ì† ì‹œê°„ (ì´ˆ)')
    
    args = parser.parse_args()
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    print_system_info()
    
    # GPU ì •ë³´ ì¶œë ¥
    if torch.cuda.is_available():
        print(f"\nğŸ® GPU ì •ë³´:")
        for i in range(torch.cuda.device_count()):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"    ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB")
            print(f"    ë©€í‹°í”„ë¡œì„¸ì„œ: {torch.cuda.get_device_properties(i).multi_processor_count}")
    
    # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘
    monitor = ResourceMonitor(interval=1)
    monitor.start_monitoring()
    
    duration_minutes = max(1, args.duration // 60)
    print(f"\nğŸš€ PyTorch GPU ì‚¬ìš©ë¥  ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ ì‹œì‘ ({duration_minutes}ë¶„)")
    print("   DCGM_FI_PROF_GR_ENGINE_ACTIVE ë©”íŠ¸ë¦­ì— ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
    print("=" * 60)
    
    # GPU ì‚¬ìš©ë¥  ìµœëŒ€í™” ê°ì²´ ìƒì„±
    gpu_workload = IntensiveGPUWorkload(duration_minutes=duration_minutes)
    
    start_time = time.time()
    
    try:
        # ì§‘ì•½ì  GPU ì›Œí¬ë¡œë“œ ì‹œì‘
        gpu_workload.start_workloads()
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ í…ŒìŠ¤íŠ¸ê°€ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
        monitor.stop_monitoring()
        monitor.print_summary()
        
        # ìµœì¢… ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸
        final_usage = monitor.get_current_usage()
        print(f"\nğŸ“Š ìµœì¢… ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰:")
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                gpu_utilization = final_usage.get(f'gpu_{i}_utilization', 0)
                gpu_memory = final_usage.get(f'gpu_{i}_memory_used', 0)
                print(f"  GPU {i} ì‚¬ìš©ë¥ : {gpu_utilization}%")
                print(f"  GPU {i} ë©”ëª¨ë¦¬: {gpu_memory:.2f} GB")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        print("\nğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        total_duration = time.time() - start_time
        print(f"\nğŸ¯ PyTorch GPU ì‚¬ìš©ë¥  ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"   ì´ ì‹¤í–‰ ì‹œê°„: {total_duration:.1f}ì´ˆ")
        print("=" * 60)


if __name__ == "__main__":
    main() 