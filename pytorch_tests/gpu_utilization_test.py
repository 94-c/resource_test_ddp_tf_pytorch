"""
PyTorch GPU ì‚¬ìš©ë¥  ì§‘ì•½ì  í…ŒìŠ¤íŠ¸
ì´ í…ŒìŠ¤íŠ¸ëŠ” GPU ì‚¬ìš©ë¥ ì„ ìµœëŒ€í™”í•˜ì—¬ GPU í™œìš©ë„ ëª¨ë‹ˆí„°ë§ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import time
import argparse
import sys
import os
import threading
import concurrent.futures
from typing import List, Dict, Optional

# Add utils directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.resource_monitor import ResourceMonitor, print_system_info


class GPUUtilizationMaximizer:
    """GPU ì‚¬ìš©ë¥  ìµœëŒ€í™” í´ë˜ìŠ¤"""
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.streams = []
        
        if not torch.cuda.is_available():
            print("âš ï¸  CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        else:
            print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name()}")
            # ì—¬ëŸ¬ CUDA ìŠ¤íŠ¸ë¦¼ ìƒì„±
            for i in range(8):
                stream = torch.cuda.Stream()
                self.streams.append(stream)
    
    def parallel_matrix_operations(self, num_operations=1000, matrix_size=2000):
        """ë³‘ë ¬ í–‰ë ¬ ì—°ì‚°ìœ¼ë¡œ GPU ì‚¬ìš©ë¥  ìµœëŒ€í™”"""
        if not torch.cuda.is_available():
            print("GPUê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        print(f"ë³‘ë ¬ í–‰ë ¬ ì—°ì‚° ì‹œì‘ ({num_operations} ì—°ì‚°, í¬ê¸°: {matrix_size})")
        
        def matrix_worker(stream_idx, operations_per_stream):
            """ê° ìŠ¤íŠ¸ë¦¼ì—ì„œ ì‹¤í–‰í•  í–‰ë ¬ ì—°ì‚°"""
            stream = self.streams[stream_idx % len(self.streams)]
            
            with torch.cuda.stream(stream):
                for i in range(operations_per_stream):
                    # ë‹¤ì–‘í•œ í–‰ë ¬ ì—°ì‚°
                    a = torch.randn(matrix_size, matrix_size, device=self.device)
                    b = torch.randn(matrix_size, matrix_size, device=self.device)
                    
                    # í–‰ë ¬ ê³±ì…ˆ
                    c = torch.matmul(a, b)
                    
                    # ê³ ìœ ê°’ ë¶„í•´ (ê³„ì‚° ì§‘ì•½ì )
                    if i % 10 == 0:
                        try:
                            eigenvalues = torch.linalg.eigvals(c[:500, :500])
                        except:
                            pass
                    
                    # ì‚¼ê°í•¨ìˆ˜ ì—°ì‚°
                    d = torch.sin(c) + torch.cos(c) + torch.tan(torch.clamp(c, -1, 1))
                    
                    # ì§€ìˆ˜ ì—°ì‚°
                    e = torch.exp(torch.clamp(d, -5, 5))
                    
                    # ë¡œê·¸ ì—°ì‚°
                    f = torch.log(torch.abs(e) + 1e-8)
                    
                    # ì—­í–‰ë ¬ ê³„ì‚°
                    try:
                        g = torch.linalg.inv(f + torch.eye(matrix_size, device=self.device) * 1e-3)
                    except:
                        g = f
                    
                    # ê²°ê³¼ ì €ì¥ (ë©”ëª¨ë¦¬ ì••ë°• ë°©ì§€ë¥¼ ìœ„í•´ ì£¼ê¸°ì  ì •ë¦¬)
                    if i % 50 == 0:
                        torch.cuda.empty_cache()
        
        # ë³‘ë ¬ ì‹¤í–‰
        operations_per_stream = num_operations // len(self.streams)
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=len(self.streams)) as executor:
            futures = []
            for i in range(len(self.streams)):
                future = executor.submit(matrix_worker, i, operations_per_stream)
                futures.append(future)
            
            # ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
            for future in concurrent.futures.as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    print(f"ìŠ¤íŠ¸ë¦¼ ì‘ì—… ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ëª¨ë“  ìŠ¤íŠ¸ë¦¼ ë™ê¸°í™”
        for stream in self.streams:
            stream.synchronize()
    
    def convolution_intensive_test(self, num_iterations=500, batch_size=32):
        """ì»¨ë³¼ë£¨ì…˜ ì§‘ì•½ì  í…ŒìŠ¤íŠ¸"""
        if not torch.cuda.is_available():
            print("GPUê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        print(f"ì»¨ë³¼ë£¨ì…˜ ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ ({num_iterations} ë°˜ë³µ, ë°°ì¹˜: {batch_size})")
        
        # ì»¨ë³¼ë£¨ì…˜ ë„¤íŠ¸ì›Œí¬ ìƒì„±
        class ConvIntensiveNet(nn.Module):
            def __init__(self):
                super(ConvIntensiveNet, self).__init__()
                self.conv_layers = nn.ModuleList([
                    nn.Conv2d(3, 64, kernel_size=3, padding=1),
                    nn.Conv2d(64, 128, kernel_size=3, padding=1),
                    nn.Conv2d(128, 256, kernel_size=3, padding=1),
                    nn.Conv2d(256, 512, kernel_size=3, padding=1),
                    nn.Conv2d(512, 1024, kernel_size=3, padding=1),
                    nn.Conv2d(1024, 512, kernel_size=3, padding=1),
                    nn.Conv2d(512, 256, kernel_size=3, padding=1),
                    nn.Conv2d(256, 128, kernel_size=3, padding=1),
                    nn.Conv2d(128, 64, kernel_size=3, padding=1),
                    nn.Conv2d(64, 3, kernel_size=3, padding=1)
                ])
                self.batch_norms = nn.ModuleList([
                    nn.BatchNorm2d(64),
                    nn.BatchNorm2d(128),
                    nn.BatchNorm2d(256),
                    nn.BatchNorm2d(512),
                    nn.BatchNorm2d(1024),
                    nn.BatchNorm2d(512),
                    nn.BatchNorm2d(256),
                    nn.BatchNorm2d(128),
                    nn.BatchNorm2d(64),
                ])
            
            def forward(self, x):
                for i, (conv, bn) in enumerate(zip(self.conv_layers[:-1], self.batch_norms)):
                    x = conv(x)
                    x = F.relu(x)
                    x = bn(x)
                    
                    # ë‹¤ìš´ìƒ˜í”Œë§ê³¼ ì—…ìƒ˜í”Œë§
                    if i < 4:
                        x = F.max_pool2d(x, 2)
                    elif i >= 5:
                        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)
                
                x = self.conv_layers[-1](x)
                return x
        
        model = ConvIntensiveNet().to(self.device)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        criterion = nn.MSELoss()
        
        for i in range(num_iterations):
            # ëœë¤ ì´ë¯¸ì§€ ë°ì´í„° ìƒì„±
            input_data = torch.randn(batch_size, 3, 256, 256, device=self.device)
            target = torch.randn(batch_size, 3, 256, 256, device=self.device)
            
            # ìˆœì „íŒŒ
            optimizer.zero_grad()
            output = model(input_data)
            loss = criterion(output, target)
            
            # ì—­ì „íŒŒ
            loss.backward()
            optimizer.step()
            
            if i % 50 == 0:
                print(f"  ì»¨ë³¼ë£¨ì…˜ ë°˜ë³µ {i}/{num_iterations}, Loss: {loss.item():.4f}")
    
    def transformer_intensive_test(self, num_iterations=200, seq_len=512, batch_size=16):
        """íŠ¸ëœìŠ¤í¬ë¨¸ ì§‘ì•½ì  í…ŒìŠ¤íŠ¸"""
        if not torch.cuda.is_available():
            print("GPUê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        print(f"íŠ¸ëœìŠ¤í¬ë¨¸ ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ ({num_iterations} ë°˜ë³µ, ì‹œí€€ìŠ¤: {seq_len}, ë°°ì¹˜: {batch_size})")
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ìƒì„±
        class IntensiveTransformer(nn.Module):
            def __init__(self, d_model=512, nhead=8, num_layers=6):
                super(IntensiveTransformer, self).__init__()
                self.d_model = d_model
                self.embedding = nn.Embedding(10000, d_model)
                self.pos_encoding = nn.Parameter(torch.randn(seq_len, d_model))
                
                encoder_layer = nn.TransformerEncoderLayer(
                    d_model=d_model,
                    nhead=nhead,
                    dim_feedforward=2048,
                    dropout=0.1,
                    batch_first=True
                )
                self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
                self.output_layer = nn.Linear(d_model, 10000)
            
            def forward(self, x):
                x = self.embedding(x)
                x = x + self.pos_encoding[:x.size(1), :]
                x = self.transformer(x)
                x = self.output_layer(x)
                return x
        
        model = IntensiveTransformer().to(self.device)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        criterion = nn.CrossEntropyLoss()
        
        for i in range(num_iterations):
            # ëœë¤ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
            input_ids = torch.randint(0, 10000, (batch_size, seq_len), device=self.device)
            target = torch.randint(0, 10000, (batch_size, seq_len), device=self.device)
            
            # ìˆœì „íŒŒ
            optimizer.zero_grad()
            output = model(input_ids)
            loss = criterion(output.view(-1, 10000), target.view(-1))
            
            # ì—­ì „íŒŒ
            loss.backward()
            optimizer.step()
            
            if i % 20 == 0:
                print(f"  íŠ¸ëœìŠ¤í¬ë¨¸ ë°˜ë³µ {i}/{num_iterations}, Loss: {loss.item():.4f}")
    
    def fft_intensive_test(self, num_iterations=1000, signal_size=8192):
        """FFT ì§‘ì•½ì  í…ŒìŠ¤íŠ¸"""
        if not torch.cuda.is_available():
            print("GPUê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        print(f"FFT ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ ({num_iterations} ë°˜ë³µ, ì‹ í˜¸ í¬ê¸°: {signal_size})")
        
        for i in range(num_iterations):
            # ë³µì¡í•œ ì‹ í˜¸ ìƒì„±
            real_signal = torch.randn(signal_size, device=self.device)
            imag_signal = torch.randn(signal_size, device=self.device)
            complex_signal = torch.complex(real_signal, imag_signal)
            
            # FFT ì—°ì‚°
            fft_result = torch.fft.fft(complex_signal)
            
            # ì—­ FFT
            ifft_result = torch.fft.ifft(fft_result)
            
            # 2D FFT (ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜)
            if i % 10 == 0:
                image_size = int(np.sqrt(signal_size))
                if image_size * image_size == signal_size:
                    image = real_signal.reshape(image_size, image_size)
                    fft_2d = torch.fft.fft2(image)
                    ifft_2d = torch.fft.ifft2(fft_2d)
            
            # ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„
            power_spectrum = torch.abs(fft_result) ** 2
            
            if i % 100 == 0:
                print(f"  FFT ë°˜ë³µ {i}/{num_iterations}")
    
    def mixed_precision_test(self, num_iterations=300):
        """í˜¼í•© ì •ë°€ë„ í…ŒìŠ¤íŠ¸"""
        if not torch.cuda.is_available():
            print("GPUê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        print(f"í˜¼í•© ì •ë°€ë„ í…ŒìŠ¤íŠ¸ ({num_iterations} ë°˜ë³µ)")
        
        # í˜¼í•© ì •ë°€ë„ ëª¨ë¸
        class MixedPrecisionModel(nn.Module):
            def __init__(self):
                super(MixedPrecisionModel, self).__init__()
                self.layers = nn.ModuleList([
                    nn.Linear(2048, 4096),
                    nn.Linear(4096, 2048),
                    nn.Linear(2048, 1024),
                    nn.Linear(1024, 512),
                    nn.Linear(512, 256),
                    nn.Linear(256, 128),
                    nn.Linear(128, 64),
                    nn.Linear(64, 10)
                ])
            
            def forward(self, x):
                for layer in self.layers[:-1]:
                    x = F.relu(layer(x))
                return self.layers[-1](x)
        
        model = MixedPrecisionModel().to(self.device)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        criterion = nn.CrossEntropyLoss()
        scaler = torch.cuda.amp.GradScaler()
        
        for i in range(num_iterations):
            # ë°°ì¹˜ ë°ì´í„° ìƒì„±
            input_data = torch.randn(128, 2048, device=self.device)
            target = torch.randint(0, 10, (128,), device=self.device)
            
            optimizer.zero_grad()
            
            # í˜¼í•© ì •ë°€ë„ ìˆœì „íŒŒ
            with torch.cuda.amp.autocast():
                output = model(input_data)
                loss = criterion(output, target)
            
            # í˜¼í•© ì •ë°€ë„ ì—­ì „íŒŒ
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            if i % 30 == 0:
                print(f"  í˜¼í•© ì •ë°€ë„ ë°˜ë³µ {i}/{num_iterations}, Loss: {loss.item():.4f}")
    
    def compute_intensive_operations(self, num_iterations=800):
        """ê³„ì‚° ì§‘ì•½ì  ì—°ì‚° í…ŒìŠ¤íŠ¸"""
        if not torch.cuda.is_available():
            print("GPUê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        print(f"ê³„ì‚° ì§‘ì•½ì  ì—°ì‚° í…ŒìŠ¤íŠ¸ ({num_iterations} ë°˜ë³µ)")
        
        for i in range(num_iterations):
            # ëŒ€ìš©ëŸ‰ í…ì„œ ìƒì„±
            size = 2000
            a = torch.randn(size, size, device=self.device)
            b = torch.randn(size, size, device=self.device)
            
            # ë³µì¡í•œ ìˆ˜í•™ ì—°ì‚°ë“¤
            c = torch.matmul(a, b)
            d = torch.sin(c) * torch.cos(c) + torch.tan(torch.clamp(c, -1, 1))
            e = torch.exp(torch.clamp(d, -5, 5))
            f = torch.log(torch.abs(e) + 1e-8)
            g = torch.sqrt(torch.abs(f) + 1e-8)
            h = torch.pow(torch.abs(g), 0.5)
            
            # í†µê³„ì  ì—°ì‚°
            mean_val = torch.mean(h)
            std_val = torch.std(h)
            max_val = torch.max(h)
            min_val = torch.min(h)
            
            # ì†ŒíŒ… ì—°ì‚°
            if i % 20 == 0:
                sorted_vals, indices = torch.sort(h.flatten())
            
            # ì¡°ê±´ë¶€ ì—°ì‚°
            mask = h > mean_val
            filtered = torch.where(mask, h, torch.zeros_like(h))
            
            if i % 80 == 0:
                print(f"  ê³„ì‚° ì§‘ì•½ì  ë°˜ë³µ {i}/{num_iterations}")


def main():
    parser = argparse.ArgumentParser(description='PyTorch GPU ì‚¬ìš©ë¥  ì§‘ì•½ì  í…ŒìŠ¤íŠ¸')
    parser.add_argument('--duration', type=int, default=240, help='í…ŒìŠ¤íŠ¸ ì§€ì† ì‹œê°„ (ì´ˆ)')
    parser.add_argument('--matrix-ops', type=int, default=500, help='í–‰ë ¬ ì—°ì‚° íšŸìˆ˜')
    parser.add_argument('--conv-iterations', type=int, default=200, help='ì»¨ë³¼ë£¨ì…˜ ë°˜ë³µ íšŸìˆ˜')
    parser.add_argument('--transformer-iterations', type=int, default=100, help='íŠ¸ëœìŠ¤í¬ë¨¸ ë°˜ë³µ íšŸìˆ˜')
    parser.add_argument('--skip-conv', action='store_true', help='ì»¨ë³¼ë£¨ì…˜ í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--skip-transformer', action='store_true', help='íŠ¸ëœìŠ¤í¬ë¨¸ í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--skip-fft', action='store_true', help='FFT í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--skip-mixed-precision', action='store_true', help='í˜¼í•© ì •ë°€ë„ í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°')
    
    args = parser.parse_args()
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    print_system_info()
    
    # GPU ì •ë³´ ì¶œë ¥
    if torch.cuda.is_available():
        print(f"\nğŸ® GPU ì •ë³´:")
        print(f"  ë””ë°”ì´ìŠ¤: {torch.cuda.get_device_name()}")
        print(f"  ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print(f"  ë©€í‹°í”„ë¡œì„¸ì„œ: {torch.cuda.get_device_properties(0).multi_processor_count}")
        print(f"  ìµœëŒ€ ìŠ¤ë ˆë“œ/ë¸”ë¡: {torch.cuda.get_device_properties(0).max_threads_per_block}")
    
    # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘
    monitor = ResourceMonitor(interval=0.5)
    monitor.start_monitoring()
    
    print(f"\nğŸš€ PyTorch GPU ì‚¬ìš©ë¥  ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ ì‹œì‘ (ì§€ì† ì‹œê°„: {args.duration}ì´ˆ)")
    print("=" * 60)
    
    # GPU ì‚¬ìš©ë¥  ìµœëŒ€í™” ê°ì²´ ìƒì„±
    gpu_maximizer = GPUUtilizationMaximizer()
    
    start_time = time.time()
    
    try:
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_results = {}
        
        # 1. ë³‘ë ¬ í–‰ë ¬ ì—°ì‚°
        print("\n1. ë³‘ë ¬ í–‰ë ¬ ì—°ì‚° í…ŒìŠ¤íŠ¸")
        gpu_maximizer.parallel_matrix_operations(
            num_operations=args.matrix_ops,
            matrix_size=1500
        )
        test_results['parallel_matrix_ops'] = True
        
        # 2. ì»¨ë³¼ë£¨ì…˜ ì§‘ì•½ì  í…ŒìŠ¤íŠ¸
        if not args.skip_conv:
            print("\n2. ì»¨ë³¼ë£¨ì…˜ ì§‘ì•½ì  í…ŒìŠ¤íŠ¸")
            gpu_maximizer.convolution_intensive_test(
                num_iterations=args.conv_iterations,
                batch_size=16
            )
            test_results['convolution_test'] = True
        
        # 3. íŠ¸ëœìŠ¤í¬ë¨¸ ì§‘ì•½ì  í…ŒìŠ¤íŠ¸
        if not args.skip_transformer:
            print("\n3. íŠ¸ëœìŠ¤í¬ë¨¸ ì§‘ì•½ì  í…ŒìŠ¤íŠ¸")
            gpu_maximizer.transformer_intensive_test(
                num_iterations=args.transformer_iterations,
                seq_len=256,
                batch_size=8
            )
            test_results['transformer_test'] = True
        
        # 4. FFT ì§‘ì•½ì  í…ŒìŠ¤íŠ¸
        if not args.skip_fft:
            print("\n4. FFT ì§‘ì•½ì  í…ŒìŠ¤íŠ¸")
            gpu_maximizer.fft_intensive_test(
                num_iterations=300,
                signal_size=4096
            )
            test_results['fft_test'] = True
        
        # 5. í˜¼í•© ì •ë°€ë„ í…ŒìŠ¤íŠ¸
        if not args.skip_mixed_precision:
            print("\n5. í˜¼í•© ì •ë°€ë„ í…ŒìŠ¤íŠ¸")
            gpu_maximizer.mixed_precision_test(num_iterations=150)
            test_results['mixed_precision_test'] = True
        
        # 6. ê³„ì‚° ì§‘ì•½ì  ì—°ì‚°
        print("\n6. ê³„ì‚° ì§‘ì•½ì  ì—°ì‚° í…ŒìŠ¤íŠ¸")
        gpu_maximizer.compute_intensive_operations(num_iterations=300)
        test_results['compute_intensive_test'] = True
        
        # ë‚¨ì€ ì‹œê°„ ë™ì•ˆ ì¶”ê°€ ë³‘ë ¬ ì—°ì‚° ìˆ˜í–‰
        elapsed_time = time.time() - start_time
        remaining_time = args.duration - elapsed_time
        
        if remaining_time > 20:
            print(f"\n7. ì¶”ê°€ ë³‘ë ¬ ì—°ì‚° ({remaining_time:.1f}ì´ˆ ë™ì•ˆ)")
            extra_operations = max(100, int(remaining_time * 5))
            gpu_maximizer.parallel_matrix_operations(
                num_operations=extra_operations,
                matrix_size=1200
            )
            test_results['extra_operations'] = True
        
    except KeyboardInterrupt:
        print("\ní…ŒìŠ¤íŠ¸ê°€ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\ní…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
        monitor.stop_monitoring()
        monitor.print_summary()
        
        # ìµœì¢… ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸
        final_usage = monitor.get_current_usage()
        print(f"\nìµœì¢… ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰:")
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                gpu_utilization = final_usage.get(f'gpu_{i}_utilization', 0)
                print(f"GPU {i} ì‚¬ìš©ë¥ : {gpu_utilization}%")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        print("\nGPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        print("\nğŸ¯ PyTorch GPU ì‚¬ìš©ë¥  ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("=" * 60)


if __name__ == "__main__":
    main() 