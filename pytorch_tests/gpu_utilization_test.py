"""
PyTorch GPU ì‚¬ìš©ë¥  ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ (DCGM ë©”íŠ¸ë¦­ ëŒ€ì‘)
ì´ í…ŒìŠ¤íŠ¸ëŠ” GPU ì‚¬ìš©ë¥ ì„ ìµœëŒ€í™”í•˜ì—¬ DCGM_FI_PROF_GR_ENGINE_ACTIVE ë©”íŠ¸ë¦­ì— ë‚˜íƒ€ë‚˜ë„ë¡ í•©ë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import time
import argparse
import sys
import os
import threading
import concurrent.futures
from typing import List, Dict, Optional
import signal
import multiprocessing as mp

# Add utils directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.resource_monitor import ResourceMonitor, print_system_info


class IntensiveGPUWorkload:
    """DCGM ë©”íŠ¸ë¦­ì— ë‚˜íƒ€ë‚˜ë„ë¡ í•˜ëŠ” ì§‘ì•½ì  GPU ì›Œí¬ë¡œë“œ"""
    
    def __init__(self, duration_minutes=10):
        self.duration_minutes = duration_minutes
        self.duration_seconds = duration_minutes * 60
        self.device_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
        self.stop_event = threading.Event()
        self.workload_threads = []
        
        if not torch.cuda.is_available():
            print("âš ï¸  CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
            return
        
        print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {self.device_count}ê°œ GPU ê°ì§€")
        for i in range(self.device_count):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
            # GPU ë©”ëª¨ë¦¬ ì •ë³´ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            try:
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"    ì´ ë©”ëª¨ë¦¬: {gpu_memory:.1f} GB")
                if torch.cuda.is_available():
                    allocated = torch.cuda.memory_allocated(i) / 1024**3
                    cached = torch.cuda.memory_reserved(i) / 1024**3
                    print(f"    í• ë‹¹ëœ ë©”ëª¨ë¦¬: {allocated:.2f} GB, ìºì‹œëœ ë©”ëª¨ë¦¬: {cached:.2f} GB")
            except Exception as e:
                print(f"    ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    def create_intensive_model(self, device_id):
        """GPU ì§‘ì•½ì  ëª¨ë¸ ìƒì„± (ì»¨í…Œì´ë„ˆ í™˜ê²½ ìµœì í™”)"""
        class SafeIntensiveModel(nn.Module):
            def __init__(self):
                super(SafeIntensiveModel, self).__init__()
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì¸ ì•ˆì „í•œ ëª¨ë¸
                self.conv_layers = nn.ModuleList([
                    nn.Conv2d(3, 64, kernel_size=5, padding=2),
                    nn.Conv2d(64, 128, kernel_size=3, padding=1),
                    nn.Conv2d(128, 256, kernel_size=3, padding=1),
                    nn.Conv2d(256, 512, kernel_size=3, padding=1),
                    nn.Conv2d(512, 256, kernel_size=3, padding=1),
                    nn.Conv2d(256, 128, kernel_size=3, padding=1),
                    nn.Conv2d(128, 64, kernel_size=3, padding=1),
                    nn.Conv2d(64, 32, kernel_size=3, padding=1),
                    nn.Conv2d(32, 3, kernel_size=3, padding=1)
                ])
                
                self.batch_norms = nn.ModuleList([
                    nn.BatchNorm2d(64),
                    nn.BatchNorm2d(128),
                    nn.BatchNorm2d(256),
                    nn.BatchNorm2d(512),
                    nn.BatchNorm2d(256),
                    nn.BatchNorm2d(128),
                    nn.BatchNorm2d(64),
                    nn.BatchNorm2d(32),
                ])
                
                # ë” ì‘ì€ Dense ë ˆì´ì–´ë“¤
                self.dense_layers = nn.ModuleList([
                    nn.Linear(256, 512),
                    nn.Linear(512, 256),
                    nn.Linear(256, 100)
                ])
                
            def forward(self, x):
                # ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ë“¤
                for i, (conv, bn) in enumerate(zip(self.conv_layers[:-1], self.batch_norms)):
                    x = conv(x)
                    x = F.relu(x)
                    x = bn(x)
                    
                    # ì ì ˆí•œ ë‹¤ìš´ìƒ˜í”Œë§
                    if i in [1, 3]:
                        x = F.max_pool2d(x, 2)
                    elif i in [5, 7]:
                        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)
                
                x = self.conv_layers[-1](x)
                
                # ê¸€ë¡œë²Œ í‰ê·  í’€ë§
                x = F.adaptive_avg_pool2d(x, (4, 4))
                x = x.view(x.size(0), -1)
                
                # Dense ë ˆì´ì–´ë“¤
                for dense in self.dense_layers:
                    x = F.relu(dense(x))
                
                return x
        
        device = torch.device(f'cuda:{device_id}')
        model = SafeIntensiveModel().to(device)
        return model, device
    
    def continuous_gpu_workload(self, device_id, workload_id):
        """ì§€ì†ì ì¸ GPU ì›Œí¬ë¡œë“œ ì‹¤í–‰ (ì•ˆì „ ë²„ì „)"""
        print(f"ğŸš€ GPU {device_id} ì›Œí¬ë¡œë“œ {workload_id} ì‹œì‘")
        
        try:
            torch.cuda.set_device(device_id)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.cuda.empty_cache()
            
            model, device = self.create_intensive_model(device_id)
            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
            criterion = nn.CrossEntropyLoss()
            
            # ë” ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¡œ ì‹œì‘
            batch_size = 8
            start_time = time.time()
            iteration = 0
            
            while not self.stop_event.is_set() and (time.time() - start_time) < self.duration_seconds:
                try:
                    # ë” ì‘ì€ ì´ë¯¸ì§€ ë°ì´í„° ìƒì„±
                    input_data = torch.randn(batch_size, 3, 256, 256, device=device)
                    target = torch.randint(0, 100, (batch_size,), device=device)
                    
                    # ìˆœì „íŒŒ
                    optimizer.zero_grad()
                    output = model(input_data)
                    loss = criterion(output, target)
                    
                    # ì—­ì „íŒŒ
                    loss.backward()
                    optimizer.step()
                    
                    # ë” ì§‘ì•½ì ì´ì§€ë§Œ ì•ˆì „í•œ ì—°ì‚°ë“¤
                    if iteration % 3 == 0:
                        # ì¤‘ê°„ í¬ê¸° í–‰ë ¬ ì—°ì‚°
                        matrix_a = torch.randn(1024, 1024, device=device)
                        matrix_b = torch.randn(1024, 1024, device=device)
                        result = torch.matmul(matrix_a, matrix_b)
                        
                        # ê°„ë‹¨í•œ FFT ì—°ì‚°
                        fft_input = torch.randn(4096, device=device)
                        fft_result = torch.fft.fft(fft_input)
                        ifft_result = torch.fft.ifft(fft_result)
                    
                    # ë” ë¹ˆë²ˆí•œ ë©”ëª¨ë¦¬ ì •ë¦¬ (5íšŒë§ˆë‹¤)
                    if iteration % 5 == 0:
                        torch.cuda.empty_cache()
                        elapsed = time.time() - start_time
                        remaining = self.duration_seconds - elapsed
                        
                        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
                        try:
                            allocated = torch.cuda.memory_allocated(device_id) / 1024**3
                            reserved = torch.cuda.memory_reserved(device_id) / 1024**3
                            print(f"  GPU {device_id} ì›Œí¬ë¡œë“œ {workload_id}: {iteration} ë°˜ë³µ ì™„ë£Œ, "
                                  f"ë‚¨ì€ ì‹œê°„: {remaining:.1f}ì´ˆ, Loss: {loss.item():.4f}, "
                                  f"ë©”ëª¨ë¦¬: {allocated:.2f}GB/{reserved:.2f}GB")
                        except Exception:
                            print(f"  GPU {device_id} ì›Œí¬ë¡œë“œ {workload_id}: {iteration} ë°˜ë³µ ì™„ë£Œ, "
                                  f"ë‚¨ì€ ì‹œê°„: {remaining:.1f}ì´ˆ, Loss: {loss.item():.4f}")
                    
                    iteration += 1
                    
                except torch.cuda.OutOfMemoryError as e:
                    print(f"  GPU {device_id} ë©”ëª¨ë¦¬ ë¶€ì¡±, ë°°ì¹˜ í¬ê¸° ì¤„ì„: {batch_size} -> {max(1, batch_size // 2)}")
                    batch_size = max(1, batch_size // 2)
                    torch.cuda.empty_cache()
                    time.sleep(1)  # ì ì‹œ ëŒ€ê¸°
                    continue
                except RuntimeError as e:
                    if "NVML" in str(e) or "CUDA" in str(e):
                        print(f"  GPU {device_id} CUDA/NVML ì˜¤ë¥˜ ë°œìƒ: {e}")
                        print(f"  ì›Œí¬ë¡œë“œë¥¼ ì•ˆì „í•˜ê²Œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                        break
                    else:
                        print(f"  GPU {device_id} ëŸ°íƒ€ì„ ì˜¤ë¥˜: {e}")
                        continue
                except Exception as e:
                    print(f"  GPU {device_id} ì›Œí¬ë¡œë“œ ì˜¤ë¥˜: {e}")
                    continue
            
            print(f"âœ… GPU {device_id} ì›Œí¬ë¡œë“œ {workload_id} ì™„ë£Œ ({iteration} ë°˜ë³µ)")
            
        except Exception as e:
            print(f"âŒ GPU {device_id} ì›Œí¬ë¡œë“œ {workload_id} ì‹¤íŒ¨: {e}")
        finally:
            # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
            try:
                torch.cuda.empty_cache()
            except Exception:
                pass
    
    def start_workloads(self):
        """ëª¨ë“  GPUì—ì„œ ì›Œí¬ë¡œë“œ ì‹œì‘ (ì•ˆì „ ë²„ì „)"""
        if not torch.cuda.is_available() or self.device_count == 0:
            print("GPUê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
            return
        
        print(f"ğŸ¯ {self.device_count}ê°œ GPUì—ì„œ {self.duration_minutes}ë¶„ ë™ì•ˆ ì§‘ì•½ì  ì›Œí¬ë¡œë“œ ì‹œì‘")
        print("   DCGM_FI_PROF_GR_ENGINE_ACTIVE ë©”íŠ¸ë¦­ì— ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
        print("   âš ï¸  ì»¨í…Œì´ë„ˆ í™˜ê²½ì—ì„œ ì•ˆì „ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        
        # ê° GPUì—ì„œ ë‹¨ì¼ ì›Œí¬ë¡œë“œë§Œ ì‹¤í–‰ (ì•ˆì „ì„± í–¥ìƒ)
        for device_id in range(self.device_count):
            thread = threading.Thread(
                target=self.continuous_gpu_workload,
                args=(device_id, 0)
            )
            thread.daemon = True
            thread.start()
            self.workload_threads.append(thread)
            
            # GPU ê°„ ì‹œì‘ ê°„ê²© (ë¦¬ì†ŒìŠ¤ ê²½í•© ë°©ì§€)
            time.sleep(2)
        
        # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ë“±ë¡
        def signal_handler(signum, frame):
            print(f"\nğŸ›‘ ì¤‘ë‹¨ ì‹ í˜¸ ë°›ìŒ. ì›Œí¬ë¡œë“œ ì¤‘ì§€ ì¤‘...")
            self.stop_event.set()
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
        # ì›Œí¬ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°
        start_time = time.time()
        try:
            while (time.time() - start_time) < self.duration_seconds and not self.stop_event.is_set():
                time.sleep(10)
                elapsed = time.time() - start_time
                remaining = self.duration_seconds - elapsed
                print(f"â±ï¸  ì§„í–‰ ì¤‘... ë‚¨ì€ ì‹œê°„: {remaining:.1f}ì´ˆ")
                
                # ì£¼ê¸°ì ìœ¼ë¡œ ì „ì²´ GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
                try:
                    for i in range(self.device_count):
                        allocated = torch.cuda.memory_allocated(i) / 1024**3
                        reserved = torch.cuda.memory_reserved(i) / 1024**3
                        print(f"    GPU {i} ë©”ëª¨ë¦¬: {allocated:.2f}GB í• ë‹¹, {reserved:.2f}GB ì˜ˆì•½")
                except Exception:
                    pass
                    
        except KeyboardInterrupt:
            print(f"\nğŸ›‘ ì‚¬ìš©ì ì¤‘ë‹¨ ìš”ì²­")
            self.stop_event.set()
        
        # ëª¨ë“  ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
        self.stop_event.set()
        for thread in self.workload_threads:
            thread.join(timeout=10)
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
        try:
            for i in range(self.device_count):
                torch.cuda.empty_cache()
        except Exception:
            pass
        
        print(f"ğŸ ëª¨ë“  GPU ì›Œí¬ë¡œë“œ ì™„ë£Œ")


def main():
    parser = argparse.ArgumentParser(description='PyTorch GPU ì‚¬ìš©ë¥  ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ (DCGM ë©”íŠ¸ë¦­ ëŒ€ì‘)')
    parser.add_argument('--duration', type=int, default=600, help='í…ŒìŠ¤íŠ¸ ì§€ì† ì‹œê°„ (ì´ˆ)')
    
    args = parser.parse_args()
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    print_system_info()
    
    # GPU ì •ë³´ ì¶œë ¥
    if torch.cuda.is_available():
        print(f"\nğŸ® GPU ì •ë³´:")
        for i in range(torch.cuda.device_count()):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"    ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB")
            print(f"    ë©€í‹°í”„ë¡œì„¸ì„œ: {torch.cuda.get_device_properties(i).multi_processor_count}")
    
    # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘
    monitor = ResourceMonitor(interval=1)
    monitor.start_monitoring()
    
    duration_minutes = max(1, args.duration // 60)
    print(f"\nğŸš€ PyTorch GPU ì‚¬ìš©ë¥  ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ ì‹œì‘ ({duration_minutes}ë¶„)")
    print("   DCGM_FI_PROF_GR_ENGINE_ACTIVE ë©”íŠ¸ë¦­ì— ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
    print("=" * 60)
    
    # GPU ì‚¬ìš©ë¥  ìµœëŒ€í™” ê°ì²´ ìƒì„±
    gpu_workload = IntensiveGPUWorkload(duration_minutes=duration_minutes)
    
    start_time = time.time()
    
    try:
        # ì§‘ì•½ì  GPU ì›Œí¬ë¡œë“œ ì‹œì‘
        gpu_workload.start_workloads()
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ í…ŒìŠ¤íŠ¸ê°€ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
        monitor.stop_monitoring()
        monitor.print_summary()
        
        # ìµœì¢… ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸
        final_usage = monitor.get_current_usage()
        print(f"\nğŸ“Š ìµœì¢… ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰:")
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                gpu_utilization = final_usage.get(f'gpu_{i}_utilization', 0)
                gpu_memory = final_usage.get(f'gpu_{i}_memory_used', 0)
                print(f"  GPU {i} ì‚¬ìš©ë¥ : {gpu_utilization}%")
                print(f"  GPU {i} ë©”ëª¨ë¦¬: {gpu_memory:.2f} GB")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        print("\nğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        total_duration = time.time() - start_time
        print(f"\nğŸ¯ PyTorch GPU ì‚¬ìš©ë¥  ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"   ì´ ì‹¤í–‰ ì‹œê°„: {total_duration:.1f}ì´ˆ")
        print("=" * 60)


if __name__ == "__main__":
    main() 