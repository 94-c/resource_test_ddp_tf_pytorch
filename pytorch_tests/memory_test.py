"""
PyTorch ë©”ëª¨ë¦¬ ì§‘ì•½ì  í…ŒìŠ¤íŠ¸
ì´ í…ŒìŠ¤íŠ¸ëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìµœëŒ€í™”í•˜ì—¬ ë©”ëª¨ë¦¬ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
import numpy as np
import time
import argparse
import sys
import os
import gc
from typing import List, Dict

# Add utils directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.resource_monitor import ResourceMonitor, print_system_info


class MemoryConsumer:
    """ë©”ëª¨ë¦¬ ì†Œë¹„ì í´ë˜ìŠ¤"""
    def __init__(self):
        self.tensor_storage = []
        self.data_cache = {}
    
    def allocate_large_tensors(self, num_tensors=100, tensor_size=(5000, 5000)):
        """ëŒ€ëŸ‰ì˜ í…ì„œë¥¼ ë©”ëª¨ë¦¬ì— í• ë‹¹"""
        print(f"Allocating {num_tensors} large tensors of size {tensor_size}")
        
        tensors = []
        for i in range(num_tensors):
            # ë‹¤ì–‘í•œ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ í…ì„œ ìƒì„±
            if i % 4 == 0:
                tensor = torch.randn(tensor_size, dtype=torch.float32)
            elif i % 4 == 1:
                tensor = torch.randn(tensor_size, dtype=torch.float64)  # ë” ë§ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©
            elif i % 4 == 2:
                tensor = torch.randint(0, 100, tensor_size, dtype=torch.int64)
            else:
                tensor = torch.ones(tensor_size, dtype=torch.float32)
            
            tensors.append(tensor)
            
            if i % 10 == 0:
                print(f"  Allocated {i+1}/{num_tensors} tensors")
        
        self.tensor_storage.extend(tensors)
        return tensors
    
    def create_memory_intensive_model(self, input_size=10000, hidden_sizes=[8192, 4096, 2048, 1024]):
        """ë©”ëª¨ë¦¬ ì§‘ì•½ì  ëª¨ë¸ ìƒì„±"""
        print(f"Creating memory intensive model with input size {input_size}")
        
        layers = []
        prev_size = input_size
        
        for i, hidden_size in enumerate(hidden_sizes):
            layers.append(nn.Linear(prev_size, hidden_size))
            layers.append(nn.ReLU())
            layers.append(nn.BatchNorm1d(hidden_size))
            layers.append(nn.Dropout(0.1))
            prev_size = hidden_size
        
        layers.append(nn.Linear(prev_size, 1000))  # ì¶œë ¥ì¸µ
        
        model = nn.Sequential(*layers)
        
        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì •ë³´ ì¶œë ¥
        total_params = sum(p.numel() for p in model.parameters())
        print(f"  Model created with {total_params:,} parameters")
        
        return model
    
    def generate_large_dataset(self, num_samples=50000, feature_size=10000):
        """ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ìƒì„±"""
        print(f"Generating large dataset: {num_samples} samples x {feature_size} features")
        
        # ë°ì´í„° ìƒì„±
        X = torch.randn(num_samples, feature_size)
        y = torch.randint(0, 1000, (num_samples,))
        
        # ì¶”ê°€ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ìœ„í•œ ë°ì´í„° ë³€í™˜
        X_normalized = torch.nn.functional.normalize(X, dim=1)
        X_squared = X ** 2
        X_expanded = torch.cat([X, X_normalized, X_squared], dim=1)
        
        dataset = {
            'X': X,
            'y': y,
            'X_normalized': X_normalized,
            'X_squared': X_squared,
            'X_expanded': X_expanded
        }
        
        self.data_cache['large_dataset'] = dataset
        return dataset
    
    def progressive_memory_allocation(self, max_iterations=100, size_multiplier=1.1):
        """ì ì§„ì  ë©”ëª¨ë¦¬ í• ë‹¹"""
        print(f"Progressive memory allocation for {max_iterations} iterations")
        
        current_size = 1000
        allocated_tensors = []
        
        for i in range(max_iterations):
            try:
                # ì ì§„ì ìœ¼ë¡œ í¬ê¸° ì¦ê°€
                tensor_size = int(current_size)
                tensor = torch.randn(tensor_size, tensor_size, dtype=torch.float32)
                allocated_tensors.append(tensor)
                
                current_size *= size_multiplier
                
                if i % 10 == 0:
                    memory_used = sum(t.numel() * t.element_size() for t in allocated_tensors)
                    print(f"  Iteration {i}: Allocated {memory_used / 1024 / 1024:.1f} MB")
                
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    print(f"  Memory limit reached at iteration {i}")
                    break
                else:
                    raise e
        
        return allocated_tensors
    
    def memory_leak_simulation(self, iterations=50):
        """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì‹œë®¬ë ˆì´ì…˜"""
        print(f"Simulating memory leak for {iterations} iterations")
        
        leaked_tensors = []
        
        for i in range(iterations):
            # ë§¤ë²ˆ ìƒˆë¡œìš´ í…ì„œ ìƒì„±í•˜ê³  ì°¸ì¡° ìœ ì§€
            tensor = torch.randn(2000, 2000, dtype=torch.float32)
            
            # ë³µì¡í•œ ì—°ì‚° ìˆ˜í–‰
            result = torch.matmul(tensor, tensor.t())
            result = torch.sin(result) + torch.cos(result)
            
            # ê²°ê³¼ë¥¼ ì €ì¥ (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì‹œë®¬ë ˆì´ì…˜)
            leaked_tensors.append(result)
            
            if i % 10 == 0:
                total_memory = sum(t.numel() * t.element_size() for t in leaked_tensors)
                print(f"  Iteration {i}: Leaked {total_memory / 1024 / 1024:.1f} MB")
        
        return leaked_tensors
    
    def clear_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        print("Clearing allocated memory...")
        self.tensor_storage.clear()
        self.data_cache.clear()
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()


def batch_processing_memory_test(batch_size=1000, num_batches=100, feature_size=5000):
    """ë°°ì¹˜ ì²˜ë¦¬ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸"""
    print(f"Batch processing memory test: {num_batches} batches of size {batch_size}")
    
    # ë©”ëª¨ë¦¬ ì§‘ì•½ì  ëª¨ë¸ ìƒì„±
    model = nn.Sequential(
        nn.Linear(feature_size, 4096),
        nn.ReLU(),
        nn.BatchNorm1d(4096),
        nn.Linear(4096, 2048),
        nn.ReLU(),
        nn.BatchNorm1d(2048),
        nn.Linear(2048, 1024),
        nn.ReLU(),
        nn.Linear(1024, 100)
    )
    
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    batch_results = []
    
    for batch_idx in range(num_batches):
        # ë°°ì¹˜ ë°ì´í„° ìƒì„±
        X = torch.randn(batch_size, feature_size)
        y = torch.randint(0, 100, (batch_size,))
        
        # ìˆœì „íŒŒ
        optimizer.zero_grad()
        output = model(X)
        loss = criterion(output, y)
        
        # ì—­ì „íŒŒ
        loss.backward()
        optimizer.step()
        
        batch_results.append({
            'batch_idx': batch_idx,
            'loss': loss.item(),
            'memory_allocated': torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
        })
        
        if batch_idx % 10 == 0:
            print(f"  Batch {batch_idx}/{num_batches}, Loss: {loss.item():.4f}")
    
    return batch_results


def tensor_operations_memory_test(num_operations=1000):
    """í…ì„œ ì—°ì‚° ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸"""
    print(f"Tensor operations memory test: {num_operations} operations")
    
    # ë‹¤ì–‘í•œ í¬ê¸°ì˜ í…ì„œë¡œ ì—°ì‚° ìˆ˜í–‰
    sizes = [1000, 2000, 3000, 4000, 5000]
    operation_results = []
    
    for i in range(num_operations):
        size = sizes[i % len(sizes)]
        
        # í…ì„œ ìƒì„±
        a = torch.randn(size, size)
        b = torch.randn(size, size)
        
        # ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì—°ì‚°
        c = torch.matmul(a, b)
        d = torch.matmul(c, a.t())
        e = torch.matmul(d, b.t())
        
        # ê²°ê³¼ ì €ì¥
        result = {
            'operation': i,
            'size': size,
            'result_mean': e.mean().item(),
            'result_std': e.std().item()
        }
        operation_results.append(result)
        
        if i % 100 == 0:
            print(f"  Operation {i}/{num_operations} completed")
    
    return operation_results


def main():
    parser = argparse.ArgumentParser(description='PyTorch ë©”ëª¨ë¦¬ ì§‘ì•½ì  í…ŒìŠ¤íŠ¸')
    parser.add_argument('--duration', type=int, default=120, help='í…ŒìŠ¤íŠ¸ ì§€ì† ì‹œê°„ (ì´ˆ)')
    parser.add_argument('--num-tensors', type=int, default=50, help='í• ë‹¹í•  í…ì„œ ìˆ˜')
    parser.add_argument('--tensor-size', type=int, default=3000, help='í…ì„œ í¬ê¸°')
    parser.add_argument('--batch-size', type=int, default=500, help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--skip-progressive', action='store_true', help='ì ì§„ì  í• ë‹¹ í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--skip-leak', action='store_true', help='ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--skip-batch', action='store_true', help='ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°')
    
    args = parser.parse_args()
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    print_system_info()
    
    # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘
    monitor = ResourceMonitor(interval=0.5)
    monitor.start_monitoring()
    
    print(f"\nğŸ§  PyTorch ë©”ëª¨ë¦¬ ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ ì‹œì‘ (ì§€ì† ì‹œê°„: {args.duration}ì´ˆ)")
    print("=" * 60)
    
    # ë©”ëª¨ë¦¬ ì†Œë¹„ì ê°ì²´ ìƒì„±
    memory_consumer = MemoryConsumer()
    
    start_time = time.time()
    
    try:
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_results = {}
        
        # 1. ëŒ€ëŸ‰ í…ì„œ í• ë‹¹
        print("\n1. ëŒ€ëŸ‰ í…ì„œ í• ë‹¹ í…ŒìŠ¤íŠ¸")
        tensors = memory_consumer.allocate_large_tensors(
            num_tensors=args.num_tensors,
            tensor_size=(args.tensor_size, args.tensor_size)
        )
        test_results['large_tensors'] = len(tensors)
        
        # 2. ë©”ëª¨ë¦¬ ì§‘ì•½ì  ëª¨ë¸ ìƒì„±
        print("\n2. ë©”ëª¨ë¦¬ ì§‘ì•½ì  ëª¨ë¸ ìƒì„±")
        model = memory_consumer.create_memory_intensive_model()
        test_results['model_params'] = sum(p.numel() for p in model.parameters())
        
        # 3. ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ìƒì„±
        print("\n3. ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ìƒì„±")
        dataset = memory_consumer.generate_large_dataset(
            num_samples=20000,
            feature_size=5000
        )
        test_results['dataset_size'] = dataset['X'].shape
        
        # 4. ì ì§„ì  ë©”ëª¨ë¦¬ í• ë‹¹
        if not args.skip_progressive:
            print("\n4. ì ì§„ì  ë©”ëª¨ë¦¬ í• ë‹¹")
            progressive_tensors = memory_consumer.progressive_memory_allocation(max_iterations=50)
            test_results['progressive_tensors'] = len(progressive_tensors)
        
        # 5. ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì‹œë®¬ë ˆì´ì…˜
        if not args.skip_leak:
            print("\n5. ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì‹œë®¬ë ˆì´ì…˜")
            leaked_tensors = memory_consumer.memory_leak_simulation(iterations=30)
            test_results['leaked_tensors'] = len(leaked_tensors)
        
        # 6. ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        if not args.skip_batch:
            print("\n6. ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
            batch_results = batch_processing_memory_test(
                batch_size=args.batch_size,
                num_batches=50,
                feature_size=2000
            )
            test_results['batch_results'] = len(batch_results)
        
        # 7. í…ì„œ ì—°ì‚° í…ŒìŠ¤íŠ¸
        print("\n7. í…ì„œ ì—°ì‚° í…ŒìŠ¤íŠ¸")
        operation_results = tensor_operations_memory_test(num_operations=200)
        test_results['operation_results'] = len(operation_results)
        
        # ë‚¨ì€ ì‹œê°„ ë™ì•ˆ ì¶”ê°€ í…ì„œ í• ë‹¹
        elapsed_time = time.time() - start_time
        remaining_time = args.duration - elapsed_time
        
        if remaining_time > 10:
            print(f"\n8. ì¶”ê°€ í…ì„œ í• ë‹¹ ({remaining_time:.1f}ì´ˆ ë™ì•ˆ)")
            extra_tensors = memory_consumer.allocate_large_tensors(
                num_tensors=max(10, int(remaining_time / 5)),
                tensor_size=(2000, 2000)
            )
            test_results['extra_tensors'] = len(extra_tensors)
        
    except KeyboardInterrupt:
        print("\ní…ŒìŠ¤íŠ¸ê°€ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\ní…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
        monitor.stop_monitoring()
        monitor.print_summary()
        
        # ìµœì¢… ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸
        final_usage = monitor.get_current_usage()
        print(f"\nìµœì¢… ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰:")
        print(f"Memory: {final_usage.get('memory_percent', 0):.1f}%")
        print(f"Memory MB: {final_usage.get('memory_mb', 0):.1f} MB")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        print("\në©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
        memory_consumer.clear_memory()
        
        print("\nğŸ¯ PyTorch ë©”ëª¨ë¦¬ ì§‘ì•½ì  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("=" * 60)


if __name__ == "__main__":
    main() 