#!/usr/bin/env python3
"""
í™˜ê²½ í™•ì¸ ë° ì•ˆì „í•œ GPU í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
NVML/CUDA ì˜¤ë¥˜ í•´ê²°ì„ ìœ„í•œ ì§„ë‹¨ ë„êµ¬
"""

import os
import sys
import subprocess

def check_environment():
    """í˜„ì¬ í™˜ê²½ ì •ë³´ í™•ì¸"""
    print("ğŸ” í™˜ê²½ ì •ë³´ í™•ì¸")
    print("=" * 50)
    
    # ê¸°ë³¸ ì‹œìŠ¤í…œ ì •ë³´
    print(f"Python ë²„ì „: {sys.version}")
    print(f"OS: {os.uname().sysname} {os.uname().release}")
    
    # PyTorch ì •ë³´
    try:
        import torch
        print(f"PyTorch ë²„ì „: {torch.__version__}")
        print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"GPU ê°œìˆ˜: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
                try:
                    props = torch.cuda.get_device_properties(i)
                    print(f"    ë©”ëª¨ë¦¬: {props.total_memory / 1024**3:.1f} GB")
                except Exception as e:
                    print(f"    ë©”ëª¨ë¦¬ ì •ë³´ ì‹¤íŒ¨: {e}")
    except ImportError:
        print("âŒ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    # NVIDIA ë“œë¼ì´ë²„ ì •ë³´
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… nvidia-smi ì‚¬ìš© ê°€ëŠ¥")
            lines = result.stdout.split('\n')
            for line in lines:
                if 'Driver Version' in line:
                    print(f"ë“œë¼ì´ë²„ ë²„ì „: {line.split('Driver Version: ')[1].split()[0]}")
                    break
        else:
            print("âŒ nvidia-smi ì‹¤í–‰ ì‹¤íŒ¨")
    except FileNotFoundError:
        print("âŒ nvidia-smië¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    print("\nğŸ”§ í™˜ê²½ ë³€ìˆ˜ í™•ì¸")
    print("=" * 50)
    env_vars = [
        'CUDA_VISIBLE_DEVICES',
        'PYTORCH_CUDA_ALLOC_CONF', 
        'CUDA_LAUNCH_BLOCKING',
        'NVIDIA_DISABLE_REQUIRE'
    ]
    
    for var in env_vars:
        value = os.environ.get(var, 'ì„¤ì •ë˜ì§€ ì•ŠìŒ')
        print(f"{var}: {value}")

def set_safe_environment():
    """ì•ˆì „í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •"""
    print("\nâš™ï¸  ì•ˆì „í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •")
    print("=" * 50)
    
    # GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256'
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    
    print("âœ… PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256")
    print("âœ… CUDA_LAUNCH_BLOCKING=1")
    
    # NVML ë¬¸ì œ í•´ê²° ì‹œë„
    try:
        import pynvml
        pynvml.nvmlInit()
        print("âœ… NVML ì´ˆê¸°í™” ì„±ê³µ")
    except Exception as e:
        print(f"âš ï¸  NVML ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        os.environ['NVIDIA_DISABLE_REQUIRE'] = '1'
        print("âœ… NVIDIA_DISABLE_REQUIRE=1 ì„¤ì •")

def run_safe_gpu_test():
    """ì•ˆì „í•œ GPU í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("\nğŸ§ª ì•ˆì „í•œ GPU í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    print("=" * 50)
    
    try:
        import torch
        if not torch.cuda.is_available():
            print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        device_count = torch.cuda.device_count()
        print(f"ğŸ¯ {device_count}ê°œ GPUì—ì„œ ì•ˆì „ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        for i in range(device_count):
            print(f"\nğŸš€ GPU {i} í…ŒìŠ¤íŠ¸ ì¤‘...")
            device = torch.device(f'cuda:{i}')
            
            try:
                # ì‘ì€ í…ì„œë¡œ í…ŒìŠ¤íŠ¸
                x = torch.randn(100, 100, device=device)
                y = torch.randn(100, 100, device=device)
                z = torch.matmul(x, y)
                
                # ë©”ëª¨ë¦¬ ì •ë³´
                allocated = torch.cuda.memory_allocated(i) / 1024**2
                reserved = torch.cuda.memory_reserved(i) / 1024**2
                
                print(f"  âœ… ê¸°ë³¸ ì—°ì‚° ì„±ê³µ")
                print(f"  ğŸ“Š ë©”ëª¨ë¦¬: {allocated:.1f}MB í• ë‹¹, {reserved:.1f}MB ì˜ˆì•½")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del x, y, z
                torch.cuda.empty_cache()
                
            except Exception as e:
                print(f"  âŒ GPU {i} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        print(f"\nâœ… ì•ˆì „ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
    except ImportError:
        print("âŒ PyTorchë¥¼ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ› ï¸  GPU í™˜ê²½ ì§„ë‹¨ ë° ì•ˆì „ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # 1. í™˜ê²½ í™•ì¸
    check_environment()
    
    # 2. ì•ˆì „í•œ í™˜ê²½ ì„¤ì •
    set_safe_environment()
    
    # 3. ì•ˆì „í•œ GPU í…ŒìŠ¤íŠ¸
    run_safe_gpu_test()
    
    # 4. ê¶Œì¥ ëª…ë ¹ì–´ ì œì‹œ
    print("\nğŸ’¡ ê¶Œì¥ í•´ê²°ì±…")
    print("=" * 50)
    print("1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •:")
    print("   export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256")
    print("   export CUDA_LAUNCH_BLOCKING=1")
    print("")
    print("2. ì•ˆì „í•œ GPU í…ŒìŠ¤íŠ¸ ì‹¤í–‰:")
    print("   python pytorch_tests/gpu_utilization_test.py --duration 300")
    print("")
    print("3. ë°°ì¹˜ í¬ê¸° ì œí•œ í…ŒìŠ¤íŠ¸:")
    print("   PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128 \\")
    print("   python run_tests.py pytorch gpu_utilization --duration 600")
    print("")
    print("4. NVML ë¬¸ì œ ì‹œ:")
    print("   export NVIDIA_DISABLE_REQUIRE=1")
    print("   python pytorch_tests/gpu_utilization_test.py --duration 300")

if __name__ == "__main__":
    main() 